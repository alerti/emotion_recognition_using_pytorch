{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1 style=\"font-size:28px;\">Facial emotions detection with PyTorch</h1></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Context</h4>\n",
    "\n",
    "<p style=\"font-size:16px;\">Emotions detection has been something  which seemed unachievable for some time, but with provision of powerful computer power, this confirms that emotion detection can now be  implemented in many areas requiring additional security or information about the person.</p>\n",
    "\n",
    "<h4>Rationale</h4>\n",
    "\n",
    "<p style=\"font-size:16px;\">This notebook confirms that through pytorch we can actually get the emotions  through just taking the picture of a person.</p>\n",
    "\n",
    "<h4>Aims and objectives</h4>\n",
    "<p style=\"font-size:16px;\">\n",
    "The aim of this project is to determine whether a person is Angry, Disgusted, scared, Happy,Sad,Surprise or Neutral based on the input data to the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3-HiuMtfIG-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../dataset/fer2013.csv')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used, fer2013.csv was extracted from one the face dtection challenges from kaggle.\n",
    "Description of the dataset.\n",
    "<ul>\n",
    "<li><h4>dataset properties</h4></li>\n",
    "<li>This dataset consists of 48x48 pixel grayscale images of faces</li>\n",
    "    <li>Breakdown of the dataset properties</li>\n",
    "<li>0: -4593 images- Angry</li>\n",
    "<li>1: -547 images- Disgust</li>\n",
    "<li>2: -5121 images- Fear</li>\n",
    "<li>3: -8989 images- Happy</li>\n",
    "<li>4: -6077 images- Sad</li>\n",
    "<li>5: -4002 images- Surprise</li>\n",
    "<li>6: -6198 images- Neutral</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CBNckGUezLC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels,\n",
    "                                   bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channeld, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.residual_conv = nn.Conv2d(in_channels=in_channeld, out_channels=out_channels, kernel_size=1, stride=2,\n",
    "                                       bias=False)\n",
    "        self.residual_bn = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "\n",
    "        self.sepConv1 = SeparableConv2d(in_channels=in_channeld, out_channels=out_channels, kernel_size=3, bias=False,\n",
    "                                        padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.sepConv2 = SeparableConv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, bias=False,\n",
    "                                        padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        self.maxp = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.residual_conv(x)\n",
    "        res = self.residual_bn(res)\n",
    "        x = self.sepConv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.sepConv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.maxp(x)\n",
    "        return res + x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(8, affine=True, momentum=0.99, eps=1e-3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(8, momentum=0.99, eps=1e-3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.module1 = ResidualBlock(in_channeld=8, out_channels=16)\n",
    "        self.module2 = ResidualBlock(in_channeld=16, out_channels=32)\n",
    "        self.module3 = ResidualBlock(in_channeld=32, out_channels=64)\n",
    "        self.module4 = ResidualBlock(in_channeld=64, out_channels=128)\n",
    "\n",
    "        self.last_conv = nn.Conv2d(in_channels=128, out_channels=num_classes, kernel_size=3, padding=1)\n",
    "        self.avgp = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.module1(x)\n",
    "        x = self.module2(x)\n",
    "        x = self.module3(x)\n",
    "        x = self.module4(x)\n",
    "        x = self.last_conv(x)\n",
    "        x = self.avgp(x)\n",
    "        x = x.view((x.shape[0], -1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.ResidualBlock:</b>\n",
    "\n",
    "<i>To create a clean code is mandatory to think about the main building blocks of the application, \n",
    "    or of the network in our case. The residual block takes an input with in_channels, applies some blocks of convolutional layers to reduce it to out_channels and sum it up to the original input. If their sizes mismatch, then \n",
    "    the input goes into an identity. We can abstract this process and create an interface that can be extended.</i><br>\n",
    "    \n",
    "<b>2.Separableconv2d.</b>\n",
    "    \n",
    "    \n",
    "<i>Separable convolutions consist of first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes the resulting output channels. The groupd argument controls how many output channels are generated per input channel in the depthwise step.</i><br>\n",
    "    \n",
    "<b>3.class Model(nn.model):</b>\n",
    "    \n",
    "This is the base class for all neural network modules in pytroch.<br>\n",
    "\n",
    "<b>4. def forward()</b><br>\n",
    "<i>\n",
    "This is a pyrtorch hook. The hook can be a forward hook or a backward hook. The forward hook will be executed when a forward call is executed. The backward hook will be executed in the backward phase.</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dQya6eRBeYE6",
    "outputId": "bded0a42-7eb6-4d51-b95f-bb86c3157614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 28709 : private val size 3589 : public val size 3589\n",
      "learning_rate: 0.01\n",
      "Epoch [1/350] Training Loss: 1.6956, Accuracy: 32.5961\n",
      "Epoch [1/350] private validation Loss: 1.6354, Accuracy: 38.2279\n",
      "Epoch [1/350] public validation Loss: 1.6702, Accuracy: 37.2249\n",
      "learning_rate: 0.01\n",
      "Epoch [2/350] Training Loss: 1.4513, Accuracy: 44.0907\n",
      "Epoch [2/350] private validation Loss: 1.3704, Accuracy: 48.2028\n",
      "Epoch [2/350] public validation Loss: 1.4136, Accuracy: 46.7540\n",
      "learning_rate: 0.01\n",
      "Epoch [3/350] Training Loss: 1.3463, Accuracy: 48.3368\n",
      "Epoch [3/350] private validation Loss: 1.3108, Accuracy: 50.9613\n",
      "Epoch [3/350] public validation Loss: 1.3278, Accuracy: 50.0139\n",
      "learning_rate: 0.01\n",
      "Epoch [4/350] Training Loss: 1.2753, Accuracy: 51.3637\n",
      "Epoch [4/350] private validation Loss: 1.2328, Accuracy: 51.3235\n",
      "Epoch [4/350] public validation Loss: 1.2734, Accuracy: 51.7693\n",
      "learning_rate: 0.01\n",
      "Epoch [5/350] Training Loss: 1.2343, Accuracy: 52.6351\n",
      "Epoch [5/350] private validation Loss: 1.2287, Accuracy: 53.1903\n",
      "Epoch [5/350] public validation Loss: 1.2458, Accuracy: 52.9395\n",
      "learning_rate: 0.01\n",
      "Epoch [6/350] Training Loss: 1.2040, Accuracy: 54.1085\n",
      "Epoch [6/350] private validation Loss: 1.1921, Accuracy: 54.6113\n",
      "Epoch [6/350] public validation Loss: 1.2107, Accuracy: 53.4411\n",
      "learning_rate: 0.01\n",
      "Epoch [7/350] Training Loss: 1.1768, Accuracy: 55.6132\n",
      "Epoch [7/350] private validation Loss: 1.1660, Accuracy: 55.8094\n",
      "Epoch [7/350] public validation Loss: 1.1819, Accuracy: 55.1128\n",
      "learning_rate: 0.01\n",
      "Epoch [8/350] Training Loss: 1.1643, Accuracy: 55.9232\n",
      "Epoch [8/350] private validation Loss: 1.1417, Accuracy: 56.1716\n",
      "Epoch [8/350] public validation Loss: 1.1940, Accuracy: 55.4472\n",
      "learning_rate: 0.01\n",
      "Epoch [9/350] Training Loss: 1.1460, Accuracy: 56.3099\n",
      "Epoch [9/350] private validation Loss: 1.1419, Accuracy: 56.3945\n",
      "Epoch [9/350] public validation Loss: 1.1607, Accuracy: 55.0571\n",
      "learning_rate: 0.01\n",
      "Epoch [10/350] Training Loss: 1.1326, Accuracy: 56.8846\n",
      "Epoch [10/350] private validation Loss: 1.1458, Accuracy: 56.6732\n",
      "Epoch [10/350] public validation Loss: 1.1767, Accuracy: 55.0571\n",
      "learning_rate: 0.01\n",
      "Epoch [11/350] Training Loss: 1.1269, Accuracy: 57.5081\n",
      "saving new model\n",
      "Epoch [11/350] private validation Loss: 1.1126, Accuracy: 57.5369\n",
      "saving new model\n",
      "Epoch [11/350] public validation Loss: 1.1449, Accuracy: 57.2583\n",
      "learning_rate: 0.01\n",
      "Epoch [12/350] Training Loss: 1.1124, Accuracy: 57.7902\n",
      "Epoch [12/350] private validation Loss: 1.1433, Accuracy: 57.9827\n",
      "saving new model\n",
      "Epoch [12/350] public validation Loss: 1.1362, Accuracy: 56.2831\n",
      "learning_rate: 0.01\n",
      "Epoch [13/350] Training Loss: 1.1055, Accuracy: 58.2291\n",
      "Epoch [13/350] private validation Loss: 1.1384, Accuracy: 56.1995\n",
      "Epoch [13/350] public validation Loss: 1.1820, Accuracy: 56.0045\n",
      "learning_rate: 0.01\n",
      "Epoch [14/350] Training Loss: 1.1045, Accuracy: 57.7937\n",
      "Epoch [14/350] private validation Loss: 1.1306, Accuracy: 57.8713\n",
      "saving new model\n",
      "Epoch [14/350] public validation Loss: 1.1294, Accuracy: 56.5339\n",
      "learning_rate: 0.01\n",
      "Epoch [15/350] Training Loss: 1.0976, Accuracy: 58.3267\n",
      "saving new model\n",
      "Epoch [15/350] private validation Loss: 1.1090, Accuracy: 56.7846\n",
      "Epoch [15/350] public validation Loss: 1.1493, Accuracy: 56.9797\n",
      "learning_rate: 0.01\n",
      "Epoch [16/350] Training Loss: 1.0894, Accuracy: 58.5844\n",
      "Epoch [16/350] private validation Loss: 1.1191, Accuracy: 57.0075\n",
      "saving new model\n",
      "Epoch [16/350] public validation Loss: 1.1156, Accuracy: 57.5369\n",
      "learning_rate: 0.01\n",
      "Epoch [17/350] Training Loss: 1.0853, Accuracy: 58.7412\n",
      "saving new model\n",
      "Epoch [17/350] private validation Loss: 1.0980, Accuracy: 57.4255\n",
      "saving new model\n",
      "Epoch [17/350] public validation Loss: 1.0965, Accuracy: 57.3697\n",
      "learning_rate: 0.01\n",
      "Epoch [18/350] Training Loss: 1.0834, Accuracy: 59.0999\n",
      "Epoch [18/350] private validation Loss: 1.1680, Accuracy: 56.5617\n",
      "Epoch [18/350] public validation Loss: 1.1926, Accuracy: 54.9178\n",
      "learning_rate: 0.01\n",
      "Epoch [19/350] Training Loss: 1.0773, Accuracy: 59.1800\n",
      "saving new model\n",
      "Epoch [19/350] private validation Loss: 1.0878, Accuracy: 58.1220\n",
      "Epoch [19/350] public validation Loss: 1.1247, Accuracy: 58.3449\n",
      "learning_rate: 0.01\n",
      "Epoch [20/350] Training Loss: 1.0775, Accuracy: 58.9711\n",
      "Epoch [20/350] private validation Loss: 1.0919, Accuracy: 58.4285\n",
      "Epoch [20/350] public validation Loss: 1.1285, Accuracy: 57.7598\n",
      "learning_rate: 0.01\n",
      "Epoch [21/350] Training Loss: 1.0755, Accuracy: 59.1626\n",
      "Epoch [21/350] private validation Loss: 1.1014, Accuracy: 58.1778\n",
      "Epoch [21/350] public validation Loss: 1.1040, Accuracy: 58.1499\n",
      "learning_rate: 0.01\n",
      "Epoch [22/350] Training Loss: 1.0760, Accuracy: 59.4413\n",
      "saving new model\n",
      "Epoch [22/350] private validation Loss: 1.0814, Accuracy: 59.9331\n",
      "saving new model\n",
      "Epoch [22/350] public validation Loss: 1.0852, Accuracy: 59.3201\n",
      "learning_rate: 0.01\n",
      "Epoch [23/350] Training Loss: 1.0698, Accuracy: 59.6364\n",
      "saving new model\n",
      "Epoch [23/350] private validation Loss: 1.0673, Accuracy: 59.0137\n",
      "Epoch [23/350] public validation Loss: 1.1017, Accuracy: 58.6514\n",
      "learning_rate: 0.01\n",
      "Epoch [24/350] Training Loss: 1.0687, Accuracy: 59.7199\n",
      "Epoch [24/350] private validation Loss: 1.1077, Accuracy: 56.7568\n",
      "Epoch [24/350] public validation Loss: 1.1504, Accuracy: 56.9797\n",
      "learning_rate: 0.01\n",
      "Epoch [25/350] Training Loss: 1.0691, Accuracy: 59.5771\n",
      "Epoch [25/350] private validation Loss: 1.1552, Accuracy: 56.2831\n",
      "Epoch [25/350] public validation Loss: 1.1582, Accuracy: 57.0354\n",
      "learning_rate: 0.01\n",
      "Epoch [26/350] Training Loss: 1.0620, Accuracy: 59.9498\n",
      "Epoch [26/350] private validation Loss: 1.1577, Accuracy: 57.0075\n",
      "Epoch [26/350] public validation Loss: 1.1337, Accuracy: 56.3388\n",
      "learning_rate: 0.01\n",
      "Epoch [27/350] Training Loss: 1.0613, Accuracy: 60.1797\n",
      "Epoch [27/350] private validation Loss: 1.0951, Accuracy: 58.1499\n",
      "Epoch [27/350] public validation Loss: 1.1131, Accuracy: 57.8713\n",
      "learning_rate: 0.01\n",
      "Epoch [28/350] Training Loss: 1.0542, Accuracy: 60.2807\n",
      "Epoch [28/350] private validation Loss: 1.1291, Accuracy: 56.6732\n",
      "Epoch [28/350] public validation Loss: 1.1289, Accuracy: 56.2274\n",
      "learning_rate: 0.01\n",
      "Epoch [29/350] Training Loss: 1.0610, Accuracy: 60.2146\n",
      "Epoch [29/350] private validation Loss: 1.0787, Accuracy: 58.9301\n",
      "Epoch [29/350] public validation Loss: 1.0899, Accuracy: 57.9270\n",
      "learning_rate: 0.01\n",
      "Epoch [30/350] Training Loss: 1.0568, Accuracy: 60.1345\n",
      "Epoch [30/350] private validation Loss: 1.0906, Accuracy: 58.8743\n",
      "Epoch [30/350] public validation Loss: 1.1694, Accuracy: 58.4285\n",
      "learning_rate: 0.01\n",
      "Epoch [31/350] Training Loss: 1.0530, Accuracy: 60.4270\n",
      "Epoch [31/350] private validation Loss: 1.0844, Accuracy: 58.5121\n",
      "saving new model\n",
      "Epoch [31/350] public validation Loss: 1.0839, Accuracy: 57.9549\n",
      "learning_rate: 0.01\n",
      "Epoch [32/350] Training Loss: 1.0541, Accuracy: 60.1379\n",
      "Epoch [32/350] private validation Loss: 1.0748, Accuracy: 59.3759\n",
      "Epoch [32/350] public validation Loss: 1.0954, Accuracy: 58.8186\n",
      "learning_rate: 0.01\n",
      "Epoch [33/350] Training Loss: 1.0505, Accuracy: 60.4201\n",
      "Epoch [33/350] private validation Loss: 1.0991, Accuracy: 57.5648\n",
      "Epoch [33/350] public validation Loss: 1.1158, Accuracy: 57.5648\n",
      "learning_rate: 0.01\n",
      "Epoch [34/350] Training Loss: 1.0481, Accuracy: 60.6256\n",
      "Epoch [34/350] private validation Loss: 1.1117, Accuracy: 58.6236\n",
      "Epoch [34/350] public validation Loss: 1.1479, Accuracy: 56.7010\n",
      "learning_rate: 0.01\n",
      "Epoch [35/350] Training Loss: 1.0484, Accuracy: 60.4131\n",
      "Epoch [35/350] private validation Loss: 1.0864, Accuracy: 57.8991\n",
      "Epoch [35/350] public validation Loss: 1.1159, Accuracy: 57.0075\n",
      "learning_rate: 0.01\n",
      "Epoch [36/350] Training Loss: 1.0471, Accuracy: 60.6395\n",
      "saving new model\n",
      "Epoch [36/350] private validation Loss: 1.0653, Accuracy: 59.2366\n",
      "Epoch [36/350] public validation Loss: 1.0874, Accuracy: 59.1251\n",
      "learning_rate: 0.01\n",
      "Epoch [37/350] Training Loss: 1.0453, Accuracy: 60.5490\n",
      "Epoch [37/350] private validation Loss: 1.0812, Accuracy: 58.8465\n",
      "Epoch [37/350] public validation Loss: 1.0900, Accuracy: 58.2335\n",
      "learning_rate: 0.01\n",
      "Epoch [38/350] Training Loss: 1.0478, Accuracy: 60.3051\n",
      "saving new model\n",
      "Epoch [38/350] private validation Loss: 1.0604, Accuracy: 58.0942\n",
      "Epoch [38/350] public validation Loss: 1.1048, Accuracy: 57.2862\n",
      "learning_rate: 0.01\n",
      "Epoch [39/350] Training Loss: 1.0394, Accuracy: 60.9286\n",
      "Epoch [39/350] private validation Loss: 1.0851, Accuracy: 58.4285\n",
      "saving new model\n",
      "Epoch [39/350] public validation Loss: 1.0838, Accuracy: 57.8434\n",
      "learning_rate: 0.01\n",
      "Epoch [40/350] Training Loss: 1.0389, Accuracy: 60.7440\n",
      "saving new model\n",
      "Epoch [40/350] private validation Loss: 1.0543, Accuracy: 59.5709\n",
      "Epoch [40/350] public validation Loss: 1.0976, Accuracy: 59.3201\n",
      "learning_rate: 0.01\n",
      "Epoch [41/350] Training Loss: 1.0375, Accuracy: 60.8172\n",
      "Epoch [41/350] private validation Loss: 1.0604, Accuracy: 59.4873\n",
      "Epoch [41/350] public validation Loss: 1.0918, Accuracy: 58.9579\n",
      "learning_rate: 0.01\n",
      "Epoch [42/350] Training Loss: 1.0396, Accuracy: 60.9495\n",
      "Epoch [42/350] private validation Loss: 1.0925, Accuracy: 58.1220\n",
      "Epoch [42/350] public validation Loss: 1.0988, Accuracy: 58.2056\n",
      "learning_rate: 0.01\n",
      "Epoch [43/350] Training Loss: 1.0421, Accuracy: 60.7614\n",
      "Epoch [43/350] private validation Loss: 1.0936, Accuracy: 58.6514\n",
      "Epoch [43/350] public validation Loss: 1.0842, Accuracy: 58.4007\n",
      "learning_rate: 0.01\n",
      "Epoch [44/350] Training Loss: 1.0357, Accuracy: 60.9983\n",
      "saving new model\n",
      "Epoch [44/350] private validation Loss: 1.0234, Accuracy: 60.6297\n",
      "saving new model\n",
      "Epoch [44/350] public validation Loss: 1.0753, Accuracy: 59.2366\n",
      "learning_rate: 0.01\n",
      "Epoch [45/350] Training Loss: 1.0385, Accuracy: 60.8694\n",
      "Epoch [45/350] private validation Loss: 1.1416, Accuracy: 57.4255\n",
      "Epoch [45/350] public validation Loss: 1.1151, Accuracy: 57.9549\n",
      "learning_rate: 0.01\n",
      "Epoch [46/350] Training Loss: 1.0382, Accuracy: 60.6918\n",
      "Epoch [46/350] private validation Loss: 1.0354, Accuracy: 60.4904\n",
      "saving new model\n",
      "Epoch [46/350] public validation Loss: 1.0404, Accuracy: 59.5988\n",
      "learning_rate: 0.01\n",
      "Epoch [47/350] Training Loss: 1.0373, Accuracy: 60.7997\n",
      "Epoch [47/350] private validation Loss: 1.1114, Accuracy: 58.5957\n",
      "Epoch [47/350] public validation Loss: 1.1224, Accuracy: 57.7320\n",
      "learning_rate: 0.01\n",
      "Epoch [48/350] Training Loss: 1.0362, Accuracy: 60.8624\n",
      "Epoch [48/350] private validation Loss: 1.0804, Accuracy: 59.0972\n",
      "Epoch [48/350] public validation Loss: 1.0718, Accuracy: 58.5121\n",
      "learning_rate: 0.01\n",
      "Epoch [49/350] Training Loss: 1.0326, Accuracy: 61.4407\n",
      "Epoch [49/350] private validation Loss: 1.0900, Accuracy: 59.5152\n",
      "Epoch [49/350] public validation Loss: 1.0988, Accuracy: 59.1808\n",
      "learning_rate: 0.01\n",
      "Epoch [50/350] Training Loss: 1.0313, Accuracy: 61.0993\n",
      "Epoch [50/350] private validation Loss: 1.0586, Accuracy: 60.2675\n",
      "Epoch [50/350] public validation Loss: 1.0995, Accuracy: 59.0137\n",
      "learning_rate: 0.01\n",
      "Epoch [51/350] Training Loss: 1.0282, Accuracy: 61.0610\n",
      "Epoch [51/350] private validation Loss: 1.0499, Accuracy: 59.0694\n",
      "Epoch [51/350] public validation Loss: 1.0951, Accuracy: 58.7072\n",
      "learning_rate: 0.01\n",
      "Epoch [52/350] Training Loss: 1.0369, Accuracy: 61.3536\n",
      "Epoch [52/350] private validation Loss: 1.0985, Accuracy: 58.2614\n",
      "Epoch [52/350] public validation Loss: 1.1320, Accuracy: 57.1468\n",
      "learning_rate: 0.01\n",
      "Epoch [53/350] Training Loss: 1.0279, Accuracy: 61.2491\n",
      "Epoch [53/350] private validation Loss: 1.0841, Accuracy: 59.4595\n",
      "Epoch [53/350] public validation Loss: 1.1186, Accuracy: 58.2335\n",
      "learning_rate: 0.01\n",
      "Epoch [54/350] Training Loss: 1.0287, Accuracy: 61.3083\n",
      "Epoch [54/350] private validation Loss: 1.0582, Accuracy: 60.0724\n",
      "Epoch [54/350] public validation Loss: 1.0789, Accuracy: 59.9053\n",
      "learning_rate: 0.01\n",
      "Epoch [55/350] Training Loss: 1.0258, Accuracy: 61.6740\n",
      "Epoch [55/350] private validation Loss: 1.0552, Accuracy: 59.3480\n",
      "Epoch [55/350] public validation Loss: 1.1069, Accuracy: 58.8186\n",
      "learning_rate: 0.01\n",
      "Epoch [56/350] Training Loss: 1.0292, Accuracy: 61.0680\n",
      "saving new model\n",
      "Epoch [56/350] private validation Loss: 1.0145, Accuracy: 60.6576\n",
      "Epoch [56/350] public validation Loss: 1.0422, Accuracy: 60.7133\n",
      "learning_rate: 0.01\n",
      "Epoch [57/350] Training Loss: 1.0228, Accuracy: 61.2735\n",
      "Epoch [57/350] private validation Loss: 1.0580, Accuracy: 59.0972\n",
      "Epoch [57/350] public validation Loss: 1.0920, Accuracy: 59.3480\n",
      "learning_rate: 0.01\n",
      "Epoch [58/350] Training Loss: 1.0276, Accuracy: 61.0749\n",
      "Epoch [58/350] private validation Loss: 1.0725, Accuracy: 59.9053\n",
      "Epoch [58/350] public validation Loss: 1.0950, Accuracy: 58.4285\n",
      "learning_rate: 0.01\n",
      "Epoch [59/350] Training Loss: 1.0272, Accuracy: 61.4163\n",
      "Epoch [59/350] private validation Loss: 1.1479, Accuracy: 57.5648\n",
      "Epoch [59/350] public validation Loss: 1.1862, Accuracy: 56.2552\n",
      "learning_rate: 0.01\n",
      "Epoch [60/350] Training Loss: 1.0288, Accuracy: 61.4372\n",
      "saving new model\n",
      "Epoch [60/350] private validation Loss: 1.0012, Accuracy: 61.1591\n",
      "Epoch [60/350] public validation Loss: 1.0738, Accuracy: 59.0694\n",
      "learning_rate: 0.01\n",
      "Epoch [61/350] Training Loss: 1.0219, Accuracy: 61.4929\n",
      "Epoch [61/350] private validation Loss: 1.0251, Accuracy: 60.1839\n",
      "Epoch [61/350] public validation Loss: 1.0574, Accuracy: 59.3480\n",
      "learning_rate: 0.01\n",
      "Epoch [62/350] Training Loss: 1.0233, Accuracy: 61.3396\n",
      "Epoch [62/350] private validation Loss: 1.0432, Accuracy: 60.1839\n",
      "saving new model\n",
      "Epoch [62/350] public validation Loss: 1.0387, Accuracy: 59.7660\n",
      "learning_rate: 0.01\n",
      "Epoch [63/350] Training Loss: 1.0262, Accuracy: 61.0680\n",
      "Epoch [63/350] private validation Loss: 1.0615, Accuracy: 59.3759\n",
      "Epoch [63/350] public validation Loss: 1.0853, Accuracy: 59.4037\n",
      "learning_rate: 0.01\n",
      "Epoch [64/350] Training Loss: 1.0286, Accuracy: 61.3640\n",
      "Epoch [64/350] private validation Loss: 1.0551, Accuracy: 59.5430\n",
      "Epoch [64/350] public validation Loss: 1.1053, Accuracy: 58.8186\n",
      "learning_rate: 0.01\n",
      "Epoch [65/350] Training Loss: 1.0218, Accuracy: 61.2143\n",
      "Epoch [65/350] private validation Loss: 1.0380, Accuracy: 60.4068\n",
      "Epoch [65/350] public validation Loss: 1.0527, Accuracy: 61.2148\n",
      "learning_rate: 0.01\n",
      "Epoch [66/350] Training Loss: 1.0233, Accuracy: 61.1516\n",
      "Epoch [66/350] private validation Loss: 1.0230, Accuracy: 61.2705\n",
      "Epoch [66/350] public validation Loss: 1.0556, Accuracy: 59.5152\n",
      "learning_rate: 0.01\n",
      "Epoch [67/350] Training Loss: 1.0234, Accuracy: 61.4093\n",
      "Epoch [67/350] private validation Loss: 1.0473, Accuracy: 59.7938\n",
      "Epoch [67/350] public validation Loss: 1.0761, Accuracy: 60.3789\n",
      "learning_rate: 0.01\n",
      "Epoch [68/350] Training Loss: 1.0188, Accuracy: 61.3571\n",
      "Epoch [68/350] private validation Loss: 1.0559, Accuracy: 60.3789\n",
      "Epoch [68/350] public validation Loss: 1.0598, Accuracy: 59.7102\n",
      "learning_rate: 0.01\n",
      "Epoch [69/350] Training Loss: 1.0176, Accuracy: 61.5626\n",
      "Epoch [69/350] private validation Loss: 1.0749, Accuracy: 60.4068\n",
      "Epoch [69/350] public validation Loss: 1.0409, Accuracy: 59.8495\n",
      "learning_rate: 0.01\n",
      "Epoch [70/350] Training Loss: 1.0173, Accuracy: 61.6880\n",
      "Epoch [70/350] private validation Loss: 1.0519, Accuracy: 58.5957\n",
      "Epoch [70/350] public validation Loss: 1.0579, Accuracy: 60.0167\n",
      "learning_rate: 0.01\n",
      "Epoch [71/350] Training Loss: 1.0190, Accuracy: 61.8308\n",
      "Epoch [71/350] private validation Loss: 1.0575, Accuracy: 58.7072\n",
      "Epoch [71/350] public validation Loss: 1.1040, Accuracy: 59.1808\n",
      "learning_rate: 0.01\n",
      "Epoch [72/350] Training Loss: 1.0202, Accuracy: 61.6497\n",
      "Epoch [72/350] private validation Loss: 1.0746, Accuracy: 58.6514\n",
      "Epoch [72/350] public validation Loss: 1.1279, Accuracy: 59.1251\n",
      "learning_rate: 0.01\n",
      "Epoch [73/350] Training Loss: 1.0238, Accuracy: 61.3257\n",
      "Epoch [73/350] private validation Loss: 1.0523, Accuracy: 60.1839\n",
      "Epoch [73/350] public validation Loss: 1.0726, Accuracy: 59.3201\n",
      "learning_rate: 0.01\n",
      "Epoch [74/350] Training Loss: 1.0153, Accuracy: 61.5068\n",
      "Epoch [74/350] private validation Loss: 1.0833, Accuracy: 59.4595\n",
      "Epoch [74/350] public validation Loss: 1.1278, Accuracy: 57.6484\n",
      "learning_rate: 0.01\n",
      "Epoch [75/350] Training Loss: 1.0142, Accuracy: 61.6218\n",
      "Epoch [75/350] private validation Loss: 1.0502, Accuracy: 59.2923\n",
      "Epoch [75/350] public validation Loss: 1.0765, Accuracy: 58.6236\n",
      "learning_rate: 0.01\n",
      "Epoch [76/350] Training Loss: 1.0260, Accuracy: 61.1550\n",
      "Epoch [76/350] private validation Loss: 1.0476, Accuracy: 59.4595\n",
      "Epoch [76/350] public validation Loss: 1.0847, Accuracy: 58.8465\n",
      "learning_rate: 0.01\n",
      "Epoch [77/350] Training Loss: 1.0202, Accuracy: 61.5870\n",
      "Epoch [77/350] private validation Loss: 1.0201, Accuracy: 60.0446\n",
      "Epoch [77/350] public validation Loss: 1.0425, Accuracy: 60.1282\n",
      "learning_rate: 0.01\n",
      "Epoch [78/350] Training Loss: 1.0175, Accuracy: 61.6079\n",
      "Epoch [78/350] private validation Loss: 1.0564, Accuracy: 60.1560\n",
      "Epoch [78/350] public validation Loss: 1.0657, Accuracy: 58.8465\n",
      "learning_rate: 0.01\n",
      "Epoch [79/350] Training Loss: 1.0184, Accuracy: 61.6636\n",
      "Epoch [79/350] private validation Loss: 1.0271, Accuracy: 60.4904\n",
      "Epoch [79/350] public validation Loss: 1.0765, Accuracy: 59.7102\n",
      "learning_rate: 0.01\n",
      "Epoch [80/350] Training Loss: 1.0099, Accuracy: 62.0015\n",
      "Epoch [80/350] private validation Loss: 1.0849, Accuracy: 59.1530\n",
      "Epoch [80/350] public validation Loss: 1.0755, Accuracy: 58.7072\n",
      "learning_rate: 0.01\n",
      "Epoch [81/350] Training Loss: 1.0191, Accuracy: 61.5904\n",
      "Epoch [81/350] private validation Loss: 1.0264, Accuracy: 60.2953\n",
      "Epoch [81/350] public validation Loss: 1.0713, Accuracy: 59.7381\n",
      "learning_rate: 0.01\n",
      "Epoch [82/350] Training Loss: 1.0167, Accuracy: 61.6462\n",
      "Epoch [82/350] private validation Loss: 1.0301, Accuracy: 60.3511\n",
      "Epoch [82/350] public validation Loss: 1.0757, Accuracy: 59.0415\n",
      "learning_rate: 0.01\n",
      "Epoch [83/350] Training Loss: 1.0142, Accuracy: 61.6392\n",
      "Epoch [83/350] private validation Loss: 1.0591, Accuracy: 59.0415\n",
      "Epoch [83/350] public validation Loss: 1.1022, Accuracy: 58.1778\n",
      "learning_rate: 0.01\n",
      "Epoch [84/350] Training Loss: 1.0163, Accuracy: 61.3083\n",
      "Epoch [84/350] private validation Loss: 1.0589, Accuracy: 59.7938\n",
      "Epoch [84/350] public validation Loss: 1.0918, Accuracy: 59.6824\n",
      "learning_rate: 0.01\n",
      "Epoch [85/350] Training Loss: 1.0146, Accuracy: 61.8726\n",
      "Epoch [85/350] private validation Loss: 1.0796, Accuracy: 59.3759\n",
      "Epoch [85/350] public validation Loss: 1.0872, Accuracy: 60.0724\n",
      "learning_rate: 0.009000000000000001\n",
      "Epoch [86/350] Training Loss: 1.0009, Accuracy: 62.1234\n",
      "Epoch [86/350] private validation Loss: 1.0647, Accuracy: 60.2396\n",
      "Epoch [86/350] public validation Loss: 1.0692, Accuracy: 59.4316\n",
      "learning_rate: 0.009000000000000001\n",
      "Epoch [87/350] Training Loss: 1.0044, Accuracy: 62.1199\n",
      "Epoch [87/350] private validation Loss: 1.0221, Accuracy: 60.3232\n",
      "Epoch [87/350] public validation Loss: 1.0588, Accuracy: 60.4625\n",
      "learning_rate: 0.009000000000000001\n",
      "Epoch [88/350] Training Loss: 1.0027, Accuracy: 62.4473\n",
      "Epoch [88/350] private validation Loss: 1.0426, Accuracy: 61.6328\n",
      "Epoch [88/350] public validation Loss: 1.0595, Accuracy: 60.6576\n",
      "learning_rate: 0.009000000000000001\n",
      "Epoch [89/350] Training Loss: 1.0042, Accuracy: 62.1896\n",
      "Epoch [89/350] private validation Loss: 1.0629, Accuracy: 60.2953\n",
      "Epoch [89/350] public validation Loss: 1.0625, Accuracy: 59.9889\n",
      "learning_rate: 0.009000000000000001\n",
      "Epoch [90/350] Training Loss: 1.0010, Accuracy: 62.3811\n",
      "Epoch [90/350] private validation Loss: 1.0195, Accuracy: 60.6018\n",
      "Epoch [90/350] public validation Loss: 1.0461, Accuracy: 60.3511\n",
      "learning_rate: 0.008100000000000001\n",
      "Epoch [91/350] Training Loss: 0.9931, Accuracy: 62.6911\n",
      "Epoch [91/350] private validation Loss: 1.0717, Accuracy: 60.4347\n",
      "Epoch [91/350] public validation Loss: 1.0883, Accuracy: 59.0972\n",
      "learning_rate: 0.008100000000000001\n",
      "Epoch [92/350] Training Loss: 0.9909, Accuracy: 62.9663\n",
      "Epoch [92/350] private validation Loss: 1.0365, Accuracy: 60.9919\n",
      "Epoch [92/350] public validation Loss: 1.0564, Accuracy: 60.4625\n",
      "learning_rate: 0.008100000000000001\n",
      "Epoch [93/350] Training Loss: 0.9913, Accuracy: 62.8688\n",
      "Epoch [93/350] private validation Loss: 1.0123, Accuracy: 61.6049\n",
      "Epoch [93/350] public validation Loss: 1.0613, Accuracy: 59.6824\n",
      "learning_rate: 0.008100000000000001\n",
      "Epoch [94/350] Training Loss: 0.9966, Accuracy: 62.3568\n",
      "Epoch [94/350] private validation Loss: 1.0583, Accuracy: 60.1560\n",
      "Epoch [94/350] public validation Loss: 1.0607, Accuracy: 59.6545\n",
      "learning_rate: 0.008100000000000001\n",
      "Epoch [95/350] Training Loss: 0.9886, Accuracy: 62.5170\n",
      "Epoch [95/350] private validation Loss: 1.0226, Accuracy: 60.5461\n",
      "Epoch [95/350] public validation Loss: 1.0704, Accuracy: 59.3201\n",
      "learning_rate: 0.007290000000000001\n",
      "Epoch [96/350] Training Loss: 0.9836, Accuracy: 63.0882\n",
      "Epoch [96/350] private validation Loss: 1.0300, Accuracy: 62.2179\n",
      "saving new model\n",
      "Epoch [96/350] public validation Loss: 1.0342, Accuracy: 59.9610\n",
      "learning_rate: 0.007290000000000001\n",
      "Epoch [97/350] Training Loss: 0.9839, Accuracy: 63.2972\n",
      "Epoch [97/350] private validation Loss: 1.0585, Accuracy: 60.7690\n",
      "Epoch [97/350] public validation Loss: 1.0639, Accuracy: 59.9889\n",
      "learning_rate: 0.007290000000000001\n",
      "Epoch [98/350] Training Loss: 0.9795, Accuracy: 63.0360\n",
      "Epoch [98/350] private validation Loss: 1.0100, Accuracy: 61.2705\n",
      "Epoch [98/350] public validation Loss: 1.0685, Accuracy: 58.8743\n",
      "learning_rate: 0.007290000000000001\n",
      "Epoch [99/350] Training Loss: 0.9850, Accuracy: 62.9489\n",
      "saving new model\n",
      "Epoch [99/350] private validation Loss: 0.9924, Accuracy: 61.8835\n",
      "saving new model\n",
      "Epoch [99/350] public validation Loss: 1.0198, Accuracy: 60.0724\n",
      "learning_rate: 0.007290000000000001\n",
      "Epoch [100/350] Training Loss: 0.9814, Accuracy: 62.9803\n",
      "Epoch [100/350] private validation Loss: 1.0269, Accuracy: 61.0755\n",
      "Epoch [100/350] public validation Loss: 1.0642, Accuracy: 59.4595\n",
      "learning_rate: 0.006561\n",
      "Epoch [101/350] Training Loss: 0.9736, Accuracy: 63.3216\n",
      "Epoch [101/350] private validation Loss: 1.0241, Accuracy: 61.5213\n",
      "Epoch [101/350] public validation Loss: 1.0601, Accuracy: 61.3541\n",
      "learning_rate: 0.006561\n",
      "Epoch [102/350] Training Loss: 0.9712, Accuracy: 63.4575\n",
      "Epoch [102/350] private validation Loss: 1.0184, Accuracy: 61.2705\n",
      "Epoch [102/350] public validation Loss: 1.0828, Accuracy: 60.9362\n",
      "learning_rate: 0.006561\n",
      "Epoch [103/350] Training Loss: 0.9667, Accuracy: 63.5132\n",
      "Epoch [103/350] private validation Loss: 1.0101, Accuracy: 61.1591\n",
      "Epoch [103/350] public validation Loss: 1.0422, Accuracy: 61.0198\n",
      "learning_rate: 0.006561\n",
      "Epoch [104/350] Training Loss: 0.9819, Accuracy: 62.9698\n",
      "Epoch [104/350] private validation Loss: 1.0295, Accuracy: 61.0755\n",
      "saving new model\n",
      "Epoch [104/350] public validation Loss: 1.0164, Accuracy: 61.1870\n",
      "learning_rate: 0.006561\n",
      "Epoch [105/350] Training Loss: 0.9661, Accuracy: 63.7640\n",
      "Epoch [105/350] private validation Loss: 1.0113, Accuracy: 60.3511\n",
      "Epoch [105/350] public validation Loss: 1.0576, Accuracy: 59.3759\n",
      "learning_rate: 0.005904900000000001\n",
      "Epoch [106/350] Training Loss: 0.9669, Accuracy: 63.8267\n",
      "saving new model\n",
      "Epoch [106/350] private validation Loss: 0.9703, Accuracy: 62.4408\n",
      "Epoch [106/350] public validation Loss: 1.0253, Accuracy: 62.2179\n",
      "learning_rate: 0.005904900000000001\n",
      "Epoch [107/350] Training Loss: 0.9649, Accuracy: 63.9660\n",
      "Epoch [107/350] private validation Loss: 0.9878, Accuracy: 62.1064\n",
      "Epoch [107/350] public validation Loss: 1.0284, Accuracy: 61.3541\n",
      "learning_rate: 0.005904900000000001\n",
      "Epoch [108/350] Training Loss: 0.9644, Accuracy: 63.8859\n",
      "Epoch [108/350] private validation Loss: 1.0061, Accuracy: 62.2736\n",
      "Epoch [108/350] public validation Loss: 1.0354, Accuracy: 60.8805\n",
      "learning_rate: 0.005904900000000001\n",
      "Epoch [109/350] Training Loss: 0.9614, Accuracy: 64.0148\n",
      "Epoch [109/350] private validation Loss: 0.9870, Accuracy: 62.7194\n",
      "saving new model\n",
      "Epoch [109/350] public validation Loss: 1.0103, Accuracy: 61.2148\n",
      "learning_rate: 0.005904900000000001\n",
      "Epoch [110/350] Training Loss: 0.9594, Accuracy: 63.7744\n",
      "Epoch [110/350] private validation Loss: 1.0184, Accuracy: 60.9919\n",
      "Epoch [110/350] public validation Loss: 1.0735, Accuracy: 60.5461\n",
      "learning_rate: 0.00531441\n",
      "Epoch [111/350] Training Loss: 0.9537, Accuracy: 64.3840\n",
      "Epoch [111/350] private validation Loss: 1.0289, Accuracy: 62.1900\n",
      "Epoch [111/350] public validation Loss: 1.0358, Accuracy: 59.7381\n",
      "learning_rate: 0.00531441\n",
      "Epoch [112/350] Training Loss: 0.9485, Accuracy: 64.2899\n",
      "Epoch [112/350] private validation Loss: 0.9919, Accuracy: 62.1064\n",
      "Epoch [112/350] public validation Loss: 1.0273, Accuracy: 61.1870\n",
      "learning_rate: 0.00531441\n",
      "Epoch [113/350] Training Loss: 0.9549, Accuracy: 64.2029\n",
      "Epoch [113/350] private validation Loss: 0.9891, Accuracy: 61.4656\n",
      "Epoch [113/350] public validation Loss: 1.0660, Accuracy: 61.4656\n",
      "learning_rate: 0.00531441\n",
      "Epoch [114/350] Training Loss: 0.9575, Accuracy: 64.0984\n",
      "Epoch [114/350] private validation Loss: 1.0006, Accuracy: 60.7690\n",
      "Epoch [114/350] public validation Loss: 1.0595, Accuracy: 60.1560\n",
      "learning_rate: 0.00531441\n",
      "Epoch [115/350] Training Loss: 0.9550, Accuracy: 64.0461\n",
      "Epoch [115/350] private validation Loss: 1.0069, Accuracy: 62.7473\n",
      "Epoch [115/350] public validation Loss: 1.0316, Accuracy: 60.4068\n",
      "learning_rate: 0.004782969000000001\n",
      "Epoch [116/350] Training Loss: 0.9431, Accuracy: 64.5547\n",
      "Epoch [116/350] private validation Loss: 1.0156, Accuracy: 62.0507\n",
      "saving new model\n",
      "Epoch [116/350] public validation Loss: 1.0023, Accuracy: 61.9114\n",
      "learning_rate: 0.004782969000000001\n",
      "Epoch [117/350] Training Loss: 0.9435, Accuracy: 64.5721\n",
      "Epoch [117/350] private validation Loss: 1.0269, Accuracy: 63.2767\n",
      "Epoch [117/350] public validation Loss: 1.0434, Accuracy: 60.5740\n",
      "learning_rate: 0.004782969000000001\n",
      "Epoch [118/350] Training Loss: 0.9453, Accuracy: 64.5268\n",
      "Epoch [118/350] private validation Loss: 1.0392, Accuracy: 61.1034\n",
      "Epoch [118/350] public validation Loss: 1.0488, Accuracy: 60.6576\n",
      "learning_rate: 0.004782969000000001\n",
      "Epoch [119/350] Training Loss: 0.9437, Accuracy: 64.6000\n",
      "Epoch [119/350] private validation Loss: 1.0025, Accuracy: 62.3015\n",
      "Epoch [119/350] public validation Loss: 1.0247, Accuracy: 61.5770\n",
      "learning_rate: 0.004782969000000001\n",
      "Epoch [120/350] Training Loss: 0.9370, Accuracy: 65.2792\n",
      "Epoch [120/350] private validation Loss: 0.9880, Accuracy: 61.7442\n",
      "Epoch [120/350] public validation Loss: 1.0514, Accuracy: 61.2705\n",
      "learning_rate: 0.004304672100000001\n",
      "Epoch [121/350] Training Loss: 0.9352, Accuracy: 64.8229\n",
      "Epoch [121/350] private validation Loss: 1.0134, Accuracy: 62.7751\n",
      "Epoch [121/350] public validation Loss: 1.0332, Accuracy: 61.9114\n",
      "learning_rate: 0.004304672100000001\n",
      "Epoch [122/350] Training Loss: 0.9327, Accuracy: 64.9831\n",
      "Epoch [122/350] private validation Loss: 0.9867, Accuracy: 63.1931\n",
      "Epoch [122/350] public validation Loss: 1.0069, Accuracy: 61.5492\n",
      "learning_rate: 0.004304672100000001\n",
      "Epoch [123/350] Training Loss: 0.9291, Accuracy: 65.0214\n",
      "Epoch [123/350] private validation Loss: 1.0222, Accuracy: 61.4099\n",
      "Epoch [123/350] public validation Loss: 1.0590, Accuracy: 60.3511\n",
      "learning_rate: 0.004304672100000001\n",
      "Epoch [124/350] Training Loss: 0.9323, Accuracy: 65.0354\n",
      "Epoch [124/350] private validation Loss: 1.0111, Accuracy: 62.2179\n",
      "Epoch [124/350] public validation Loss: 1.0395, Accuracy: 59.8495\n",
      "learning_rate: 0.004304672100000001\n",
      "Epoch [125/350] Training Loss: 0.9329, Accuracy: 65.0005\n",
      "Epoch [125/350] private validation Loss: 1.0240, Accuracy: 62.8587\n",
      "Epoch [125/350] public validation Loss: 1.0100, Accuracy: 60.6297\n",
      "learning_rate: 0.003874204890000001\n",
      "Epoch [126/350] Training Loss: 0.9294, Accuracy: 65.0946\n",
      "Epoch [126/350] private validation Loss: 1.0178, Accuracy: 62.5801\n",
      "Epoch [126/350] public validation Loss: 1.0258, Accuracy: 60.7133\n",
      "learning_rate: 0.003874204890000001\n",
      "Epoch [127/350] Training Loss: 0.9253, Accuracy: 65.0702\n",
      "Epoch [127/350] private validation Loss: 1.0011, Accuracy: 61.9671\n",
      "Epoch [127/350] public validation Loss: 1.0292, Accuracy: 60.9919\n",
      "learning_rate: 0.003874204890000001\n",
      "Epoch [128/350] Training Loss: 0.9231, Accuracy: 65.3767\n",
      "Epoch [128/350] private validation Loss: 0.9703, Accuracy: 63.0259\n",
      "saving new model\n",
      "Epoch [128/350] public validation Loss: 0.9951, Accuracy: 62.4129\n",
      "learning_rate: 0.003874204890000001\n",
      "Epoch [129/350] Training Loss: 0.9259, Accuracy: 65.1851\n",
      "saving new model\n",
      "Epoch [129/350] private validation Loss: 0.9644, Accuracy: 62.5244\n",
      "Epoch [129/350] public validation Loss: 1.0323, Accuracy: 62.1622\n",
      "learning_rate: 0.003874204890000001\n",
      "Epoch [130/350] Training Loss: 0.9230, Accuracy: 65.4499\n",
      "Epoch [130/350] private validation Loss: 0.9837, Accuracy: 63.0259\n",
      "Epoch [130/350] public validation Loss: 1.0130, Accuracy: 60.4904\n",
      "learning_rate: 0.003486784401000001\n",
      "Epoch [131/350] Training Loss: 0.9198, Accuracy: 65.4429\n",
      "Epoch [131/350] private validation Loss: 1.0090, Accuracy: 61.3541\n",
      "Epoch [131/350] public validation Loss: 1.0264, Accuracy: 60.6854\n",
      "learning_rate: 0.003486784401000001\n",
      "Epoch [132/350] Training Loss: 0.9149, Accuracy: 65.7285\n",
      "Epoch [132/350] private validation Loss: 1.0089, Accuracy: 63.8897\n",
      "Epoch [132/350] public validation Loss: 1.0277, Accuracy: 61.0755\n",
      "learning_rate: 0.003486784401000001\n",
      "Epoch [133/350] Training Loss: 0.9186, Accuracy: 65.7738\n",
      "Epoch [133/350] private validation Loss: 0.9691, Accuracy: 63.4717\n",
      "Epoch [133/350] public validation Loss: 1.0406, Accuracy: 61.7164\n",
      "learning_rate: 0.003486784401000001\n",
      "Epoch [134/350] Training Loss: 0.9138, Accuracy: 65.5474\n",
      "Epoch [134/350] private validation Loss: 0.9859, Accuracy: 62.2458\n",
      "Epoch [134/350] public validation Loss: 1.0296, Accuracy: 61.0755\n",
      "learning_rate: 0.003486784401000001\n",
      "Epoch [135/350] Training Loss: 0.9108, Accuracy: 65.6763\n",
      "Epoch [135/350] private validation Loss: 1.0013, Accuracy: 62.5801\n",
      "Epoch [135/350] public validation Loss: 1.0124, Accuracy: 61.3820\n",
      "learning_rate: 0.0031381059609000006\n",
      "Epoch [136/350] Training Loss: 0.9101, Accuracy: 65.9619\n",
      "Epoch [136/350] private validation Loss: 1.0357, Accuracy: 62.6916\n",
      "Epoch [136/350] public validation Loss: 1.0422, Accuracy: 60.2953\n",
      "learning_rate: 0.0031381059609000006\n",
      "Epoch [137/350] Training Loss: 0.9118, Accuracy: 65.8017\n",
      "saving new model\n",
      "Epoch [137/350] private validation Loss: 0.9615, Accuracy: 64.4469\n",
      "Epoch [137/350] public validation Loss: 1.0049, Accuracy: 62.1343\n",
      "learning_rate: 0.0031381059609000006\n",
      "Epoch [138/350] Training Loss: 0.9090, Accuracy: 66.1326\n",
      "Epoch [138/350] private validation Loss: 0.9739, Accuracy: 62.7751\n",
      "Epoch [138/350] public validation Loss: 1.0489, Accuracy: 61.4935\n",
      "learning_rate: 0.0031381059609000006\n",
      "Epoch [139/350] Training Loss: 0.9052, Accuracy: 66.2858\n",
      "saving new model\n",
      "Epoch [139/350] private validation Loss: 0.9513, Accuracy: 63.7225\n",
      "Epoch [139/350] public validation Loss: 1.0341, Accuracy: 60.4625\n",
      "learning_rate: 0.0031381059609000006\n",
      "Epoch [140/350] Training Loss: 0.9087, Accuracy: 65.8853\n",
      "Epoch [140/350] private validation Loss: 0.9907, Accuracy: 63.4160\n",
      "Epoch [140/350] public validation Loss: 1.0326, Accuracy: 61.4099\n",
      "learning_rate: 0.0028242953648100013\n",
      "Epoch [141/350] Training Loss: 0.9048, Accuracy: 66.2092\n",
      "Epoch [141/350] private validation Loss: 0.9934, Accuracy: 62.8309\n",
      "Epoch [141/350] public validation Loss: 1.0491, Accuracy: 60.4347\n",
      "learning_rate: 0.0028242953648100013\n",
      "Epoch [142/350] Training Loss: 0.9018, Accuracy: 66.1570\n",
      "Epoch [142/350] private validation Loss: 1.0423, Accuracy: 62.6358\n",
      "Epoch [142/350] public validation Loss: 1.0675, Accuracy: 60.7969\n",
      "learning_rate: 0.0028242953648100013\n",
      "Epoch [143/350] Training Loss: 0.9002, Accuracy: 66.0559\n",
      "Epoch [143/350] private validation Loss: 1.0279, Accuracy: 62.2179\n",
      "Epoch [143/350] public validation Loss: 1.0420, Accuracy: 61.9393\n",
      "learning_rate: 0.0028242953648100013\n",
      "Epoch [144/350] Training Loss: 0.8992, Accuracy: 66.2719\n",
      "Epoch [144/350] private validation Loss: 0.9805, Accuracy: 63.1931\n",
      "Epoch [144/350] public validation Loss: 1.0249, Accuracy: 62.3572\n",
      "learning_rate: 0.0028242953648100013\n",
      "Epoch [145/350] Training Loss: 0.8997, Accuracy: 66.4252\n",
      "saving new model\n",
      "Epoch [145/350] private validation Loss: 0.9464, Accuracy: 64.4191\n",
      "Epoch [145/350] public validation Loss: 0.9966, Accuracy: 62.3572\n",
      "learning_rate: 0.002541865828329001\n",
      "Epoch [146/350] Training Loss: 0.8945, Accuracy: 66.4739\n",
      "Epoch [146/350] private validation Loss: 0.9751, Accuracy: 63.7225\n",
      "Epoch [146/350] public validation Loss: 1.0051, Accuracy: 62.1622\n",
      "learning_rate: 0.002541865828329001\n",
      "Epoch [147/350] Training Loss: 0.8924, Accuracy: 66.6516\n",
      "Epoch [147/350] private validation Loss: 0.9727, Accuracy: 63.6110\n",
      "Epoch [147/350] public validation Loss: 1.0249, Accuracy: 62.2458\n",
      "learning_rate: 0.002541865828329001\n",
      "Epoch [148/350] Training Loss: 0.8977, Accuracy: 66.6829\n",
      "Epoch [148/350] private validation Loss: 0.9883, Accuracy: 62.7473\n",
      "Epoch [148/350] public validation Loss: 1.0017, Accuracy: 62.2736\n",
      "learning_rate: 0.002541865828329001\n",
      "Epoch [149/350] Training Loss: 0.8913, Accuracy: 66.4565\n",
      "Epoch [149/350] private validation Loss: 0.9665, Accuracy: 63.6110\n",
      "Epoch [149/350] public validation Loss: 1.0101, Accuracy: 62.5522\n",
      "learning_rate: 0.002541865828329001\n",
      "Epoch [150/350] Training Loss: 0.8940, Accuracy: 66.5610\n",
      "Epoch [150/350] private validation Loss: 0.9765, Accuracy: 63.7225\n",
      "Epoch [150/350] public validation Loss: 1.0215, Accuracy: 61.7164\n",
      "learning_rate: 0.002287679245496101\n",
      "Epoch [151/350] Training Loss: 0.8839, Accuracy: 67.0382\n",
      "Epoch [151/350] private validation Loss: 0.9541, Accuracy: 64.2240\n",
      "saving new model\n",
      "Epoch [151/350] public validation Loss: 0.9854, Accuracy: 62.0228\n",
      "learning_rate: 0.002287679245496101\n",
      "Epoch [152/350] Training Loss: 0.8889, Accuracy: 66.6899\n",
      "Epoch [152/350] private validation Loss: 0.9660, Accuracy: 63.3045\n",
      "Epoch [152/350] public validation Loss: 1.0245, Accuracy: 61.3541\n",
      "learning_rate: 0.002287679245496101\n",
      "Epoch [153/350] Training Loss: 0.8824, Accuracy: 66.9058\n",
      "Epoch [153/350] private validation Loss: 0.9783, Accuracy: 63.3603\n",
      "Epoch [153/350] public validation Loss: 1.0599, Accuracy: 62.0507\n",
      "learning_rate: 0.002287679245496101\n",
      "Epoch [154/350] Training Loss: 0.8849, Accuracy: 66.9511\n",
      "Epoch [154/350] private validation Loss: 0.9805, Accuracy: 64.0847\n",
      "Epoch [154/350] public validation Loss: 1.0006, Accuracy: 61.5492\n",
      "learning_rate: 0.002287679245496101\n",
      "Epoch [155/350] Training Loss: 0.8805, Accuracy: 67.1775\n",
      "Epoch [155/350] private validation Loss: 0.9901, Accuracy: 63.3881\n",
      "Epoch [155/350] public validation Loss: 1.0155, Accuracy: 62.1622\n",
      "learning_rate: 0.002058911320946491\n",
      "Epoch [156/350] Training Loss: 0.8806, Accuracy: 67.1915\n",
      "Epoch [156/350] private validation Loss: 1.0016, Accuracy: 62.7194\n",
      "Epoch [156/350] public validation Loss: 1.0336, Accuracy: 60.9919\n",
      "learning_rate: 0.002058911320946491\n",
      "Epoch [157/350] Training Loss: 0.8762, Accuracy: 67.2611\n",
      "Epoch [157/350] private validation Loss: 0.9805, Accuracy: 63.0538\n",
      "Epoch [157/350] public validation Loss: 1.0184, Accuracy: 61.9671\n",
      "learning_rate: 0.002058911320946491\n",
      "Epoch [158/350] Training Loss: 0.8782, Accuracy: 67.2298\n",
      "Epoch [158/350] private validation Loss: 0.9715, Accuracy: 63.5832\n",
      "Epoch [158/350] public validation Loss: 1.0465, Accuracy: 61.5213\n",
      "learning_rate: 0.002058911320946491\n",
      "Epoch [159/350] Training Loss: 0.8774, Accuracy: 67.1427\n",
      "Epoch [159/350] private validation Loss: 0.9592, Accuracy: 62.8030\n",
      "Epoch [159/350] public validation Loss: 1.0399, Accuracy: 61.5770\n",
      "learning_rate: 0.002058911320946491\n",
      "Epoch [160/350] Training Loss: 0.8799, Accuracy: 67.0765\n",
      "Epoch [160/350] private validation Loss: 0.9614, Accuracy: 64.6420\n",
      "Epoch [160/350] public validation Loss: 1.0115, Accuracy: 63.2210\n",
      "learning_rate: 0.0018530201888518416\n",
      "Epoch [161/350] Training Loss: 0.8735, Accuracy: 67.3726\n",
      "Epoch [161/350] private validation Loss: 0.9476, Accuracy: 63.3045\n",
      "Epoch [161/350] public validation Loss: 1.0125, Accuracy: 62.2458\n",
      "learning_rate: 0.0018530201888518416\n",
      "Epoch [162/350] Training Loss: 0.8688, Accuracy: 67.7000\n",
      "Epoch [162/350] private validation Loss: 0.9585, Accuracy: 63.8618\n",
      "Epoch [162/350] public validation Loss: 0.9927, Accuracy: 61.9114\n",
      "learning_rate: 0.0018530201888518416\n",
      "Epoch [163/350] Training Loss: 0.8755, Accuracy: 67.0939\n",
      "Epoch [163/350] private validation Loss: 0.9508, Accuracy: 63.6110\n",
      "Epoch [163/350] public validation Loss: 1.0466, Accuracy: 61.8835\n",
      "learning_rate: 0.0018530201888518416\n",
      "Epoch [164/350] Training Loss: 0.8712, Accuracy: 67.2995\n",
      "Epoch [164/350] private validation Loss: 0.9920, Accuracy: 63.3603\n",
      "Epoch [164/350] public validation Loss: 1.0641, Accuracy: 62.0228\n",
      "learning_rate: 0.0018530201888518416\n",
      "Epoch [165/350] Training Loss: 0.8729, Accuracy: 67.2019\n",
      "saving new model\n",
      "Epoch [165/350] private validation Loss: 0.9422, Accuracy: 64.5862\n",
      "Epoch [165/350] public validation Loss: 1.0120, Accuracy: 63.0259\n",
      "learning_rate: 0.0016677181699666576\n",
      "Epoch [166/350] Training Loss: 0.8646, Accuracy: 67.4388\n",
      "Epoch [166/350] private validation Loss: 0.9821, Accuracy: 63.2767\n",
      "Epoch [166/350] public validation Loss: 1.0665, Accuracy: 61.0198\n",
      "learning_rate: 0.0016677181699666576\n",
      "Epoch [167/350] Training Loss: 0.8652, Accuracy: 67.3064\n",
      "Epoch [167/350] private validation Loss: 0.9581, Accuracy: 64.5305\n",
      "saving new model\n",
      "Epoch [167/350] public validation Loss: 0.9817, Accuracy: 62.9145\n",
      "learning_rate: 0.0016677181699666576\n",
      "Epoch [168/350] Training Loss: 0.8655, Accuracy: 67.6722\n",
      "Epoch [168/350] private validation Loss: 0.9442, Accuracy: 64.1683\n",
      "Epoch [168/350] public validation Loss: 0.9935, Accuracy: 62.4965\n",
      "learning_rate: 0.0016677181699666576\n",
      "Epoch [169/350] Training Loss: 0.8629, Accuracy: 67.5502\n",
      "saving new model\n",
      "Epoch [169/350] private validation Loss: 0.9295, Accuracy: 64.3076\n",
      "Epoch [169/350] public validation Loss: 1.0040, Accuracy: 62.3851\n",
      "learning_rate: 0.0016677181699666576\n",
      "Epoch [170/350] Training Loss: 0.8644, Accuracy: 67.6513\n",
      "Epoch [170/350] private validation Loss: 0.9674, Accuracy: 63.7225\n",
      "Epoch [170/350] public validation Loss: 1.0172, Accuracy: 61.8835\n",
      "learning_rate: 0.0015009463529699917\n",
      "Epoch [171/350] Training Loss: 0.8578, Accuracy: 68.3375\n",
      "Epoch [171/350] private validation Loss: 0.9497, Accuracy: 64.3076\n",
      "Epoch [171/350] public validation Loss: 1.0084, Accuracy: 61.7721\n",
      "learning_rate: 0.0015009463529699917\n",
      "Epoch [172/350] Training Loss: 0.8590, Accuracy: 68.0483\n",
      "Epoch [172/350] private validation Loss: 0.9666, Accuracy: 64.6977\n",
      "Epoch [172/350] public validation Loss: 1.0175, Accuracy: 62.3572\n",
      "learning_rate: 0.0015009463529699917\n",
      "Epoch [173/350] Training Loss: 0.8573, Accuracy: 68.0379\n",
      "Epoch [173/350] private validation Loss: 0.9358, Accuracy: 64.4469\n",
      "Epoch [173/350] public validation Loss: 1.0068, Accuracy: 62.5522\n",
      "learning_rate: 0.0015009463529699917\n",
      "Epoch [174/350] Training Loss: 0.8599, Accuracy: 67.7592\n",
      "Epoch [174/350] private validation Loss: 0.9645, Accuracy: 63.7782\n",
      "Epoch [174/350] public validation Loss: 1.0396, Accuracy: 62.4687\n",
      "learning_rate: 0.0015009463529699917\n",
      "Epoch [175/350] Training Loss: 0.8570, Accuracy: 68.4071\n",
      "Epoch [175/350] private validation Loss: 0.9522, Accuracy: 63.3324\n",
      "Epoch [175/350] public validation Loss: 0.9935, Accuracy: 62.3015\n",
      "learning_rate: 0.0013508517176729928\n",
      "Epoch [176/350] Training Loss: 0.8568, Accuracy: 68.0901\n",
      "Epoch [176/350] private validation Loss: 0.9543, Accuracy: 64.2240\n",
      "Epoch [176/350] public validation Loss: 1.0016, Accuracy: 63.0538\n",
      "learning_rate: 0.0013508517176729928\n",
      "Epoch [177/350] Training Loss: 0.8544, Accuracy: 68.0658\n",
      "Epoch [177/350] private validation Loss: 0.9817, Accuracy: 64.2519\n",
      "Epoch [177/350] public validation Loss: 0.9925, Accuracy: 61.8557\n",
      "learning_rate: 0.0013508517176729928\n",
      "Epoch [178/350] Training Loss: 0.8533, Accuracy: 68.1772\n",
      "Epoch [178/350] private validation Loss: 0.9687, Accuracy: 63.6668\n",
      "Epoch [178/350] public validation Loss: 0.9946, Accuracy: 63.2210\n",
      "learning_rate: 0.0013508517176729928\n",
      "Epoch [179/350] Training Loss: 0.8503, Accuracy: 68.3340\n",
      "Epoch [179/350] private validation Loss: 1.0003, Accuracy: 63.0816\n",
      "Epoch [179/350] public validation Loss: 1.0273, Accuracy: 61.9950\n",
      "learning_rate: 0.0013508517176729928\n",
      "Epoch [180/350] Training Loss: 0.8522, Accuracy: 68.3026\n",
      "Epoch [180/350] private validation Loss: 0.9623, Accuracy: 64.0568\n",
      "Epoch [180/350] public validation Loss: 1.0225, Accuracy: 62.1622\n",
      "learning_rate: 0.0012157665459056935\n",
      "Epoch [181/350] Training Loss: 0.8458, Accuracy: 68.2991\n",
      "Epoch [181/350] private validation Loss: 0.9405, Accuracy: 63.6668\n",
      "Epoch [181/350] public validation Loss: 1.0079, Accuracy: 61.7442\n",
      "learning_rate: 0.0012157665459056935\n",
      "Epoch [182/350] Training Loss: 0.8470, Accuracy: 68.5360\n",
      "Epoch [182/350] private validation Loss: 0.9317, Accuracy: 64.8649\n",
      "saving new model\n",
      "Epoch [182/350] public validation Loss: 0.9661, Accuracy: 63.0259\n",
      "learning_rate: 0.0012157665459056935\n",
      "Epoch [183/350] Training Loss: 0.8458, Accuracy: 68.6927\n",
      "Epoch [183/350] private validation Loss: 0.9659, Accuracy: 63.1931\n",
      "Epoch [183/350] public validation Loss: 1.0618, Accuracy: 62.3293\n",
      "learning_rate: 0.0012157665459056935\n",
      "Epoch [184/350] Training Loss: 0.8485, Accuracy: 68.2957\n",
      "Epoch [184/350] private validation Loss: 0.9344, Accuracy: 64.6141\n",
      "Epoch [184/350] public validation Loss: 1.0140, Accuracy: 62.6358\n",
      "learning_rate: 0.0012157665459056935\n",
      "Epoch [185/350] Training Loss: 0.8437, Accuracy: 68.5673\n",
      "Epoch [185/350] private validation Loss: 0.9363, Accuracy: 64.5862\n",
      "Epoch [185/350] public validation Loss: 0.9912, Accuracy: 62.7194\n",
      "learning_rate: 0.0010941898913151243\n",
      "Epoch [186/350] Training Loss: 0.8386, Accuracy: 68.6544\n",
      "Epoch [186/350] private validation Loss: 1.0250, Accuracy: 64.0011\n",
      "Epoch [186/350] public validation Loss: 1.0646, Accuracy: 62.3851\n",
      "learning_rate: 0.0010941898913151243\n",
      "Epoch [187/350] Training Loss: 0.8426, Accuracy: 68.3758\n",
      "Epoch [187/350] private validation Loss: 0.9629, Accuracy: 63.8618\n",
      "Epoch [187/350] public validation Loss: 1.0356, Accuracy: 61.6049\n",
      "learning_rate: 0.0010941898913151243\n",
      "Epoch [188/350] Training Loss: 0.8382, Accuracy: 68.6614\n",
      "Epoch [188/350] private validation Loss: 0.9518, Accuracy: 64.5026\n",
      "Epoch [188/350] public validation Loss: 1.0219, Accuracy: 62.6358\n",
      "learning_rate: 0.0010941898913151243\n",
      "Epoch [189/350] Training Loss: 0.8406, Accuracy: 68.8913\n",
      "Epoch [189/350] private validation Loss: 0.9781, Accuracy: 64.9206\n",
      "Epoch [189/350] public validation Loss: 0.9804, Accuracy: 62.9702\n",
      "learning_rate: 0.0010941898913151243\n",
      "Epoch [190/350] Training Loss: 0.8387, Accuracy: 68.7276\n",
      "Epoch [190/350] private validation Loss: 0.9901, Accuracy: 64.0290\n",
      "Epoch [190/350] public validation Loss: 0.9870, Accuracy: 62.8309\n",
      "learning_rate: 0.0009847709021836117\n",
      "Epoch [191/350] Training Loss: 0.8357, Accuracy: 68.7206\n",
      "Epoch [191/350] private validation Loss: 0.9395, Accuracy: 64.3076\n",
      "Epoch [191/350] public validation Loss: 0.9780, Accuracy: 62.9423\n",
      "learning_rate: 0.0009847709021836117\n",
      "Epoch [192/350] Training Loss: 0.8274, Accuracy: 69.1630\n",
      "Epoch [192/350] private validation Loss: 0.9778, Accuracy: 63.9454\n",
      "Epoch [192/350] public validation Loss: 0.9866, Accuracy: 62.5244\n",
      "learning_rate: 0.0009847709021836117\n",
      "Epoch [193/350] Training Loss: 0.8342, Accuracy: 68.9993\n",
      "Epoch [193/350] private validation Loss: 0.9515, Accuracy: 64.5026\n",
      "Epoch [193/350] public validation Loss: 0.9957, Accuracy: 62.2736\n",
      "learning_rate: 0.0009847709021836117\n",
      "Epoch [194/350] Training Loss: 0.8321, Accuracy: 69.1107\n",
      "Epoch [194/350] private validation Loss: 0.9448, Accuracy: 64.4191\n",
      "Epoch [194/350] public validation Loss: 1.0241, Accuracy: 62.6358\n",
      "learning_rate: 0.0009847709021836117\n",
      "Epoch [195/350] Training Loss: 0.8324, Accuracy: 68.9505\n",
      "Epoch [195/350] private validation Loss: 0.9360, Accuracy: 64.7256\n",
      "Epoch [195/350] public validation Loss: 0.9857, Accuracy: 62.3851\n",
      "learning_rate: 0.0008862938119652507\n",
      "Epoch [196/350] Training Loss: 0.8303, Accuracy: 69.1212\n",
      "Epoch [196/350] private validation Loss: 0.9664, Accuracy: 65.4221\n",
      "Epoch [196/350] public validation Loss: 1.0051, Accuracy: 62.4687\n",
      "learning_rate: 0.0008862938119652507\n",
      "Epoch [197/350] Training Loss: 0.8253, Accuracy: 69.4103\n",
      "Epoch [197/350] private validation Loss: 0.9517, Accuracy: 64.4748\n",
      "Epoch [197/350] public validation Loss: 1.0339, Accuracy: 61.6606\n",
      "learning_rate: 0.0008862938119652507\n",
      "Epoch [198/350] Training Loss: 0.8288, Accuracy: 69.2501\n",
      "Epoch [198/350] private validation Loss: 0.9579, Accuracy: 64.5584\n",
      "Epoch [198/350] public validation Loss: 1.0052, Accuracy: 62.4129\n",
      "learning_rate: 0.0008862938119652507\n",
      "Epoch [199/350] Training Loss: 0.8268, Accuracy: 69.0724\n",
      "Epoch [199/350] private validation Loss: 0.9955, Accuracy: 63.0538\n",
      "Epoch [199/350] public validation Loss: 1.0405, Accuracy: 61.3263\n",
      "learning_rate: 0.0008862938119652507\n",
      "Epoch [200/350] Training Loss: 0.8253, Accuracy: 69.2292\n",
      "Epoch [200/350] private validation Loss: 0.9608, Accuracy: 64.2240\n",
      "Epoch [200/350] public validation Loss: 0.9924, Accuracy: 61.9950\n",
      "learning_rate: 0.0007976644307687256\n",
      "Epoch [201/350] Training Loss: 0.8204, Accuracy: 69.4207\n",
      "Epoch [201/350] private validation Loss: 0.9485, Accuracy: 64.3912\n",
      "Epoch [201/350] public validation Loss: 1.0051, Accuracy: 62.8587\n",
      "learning_rate: 0.0007976644307687256\n",
      "Epoch [202/350] Training Loss: 0.8270, Accuracy: 69.2884\n",
      "Epoch [202/350] private validation Loss: 0.9704, Accuracy: 64.2240\n",
      "Epoch [202/350] public validation Loss: 1.0262, Accuracy: 62.5801\n",
      "learning_rate: 0.0007976644307687256\n",
      "Epoch [203/350] Training Loss: 0.8261, Accuracy: 69.3302\n",
      "Epoch [203/350] private validation Loss: 0.9495, Accuracy: 63.5553\n",
      "Epoch [203/350] public validation Loss: 0.9882, Accuracy: 62.7194\n",
      "learning_rate: 0.0007976644307687256\n",
      "Epoch [204/350] Training Loss: 0.8218, Accuracy: 69.4312\n",
      "Epoch [204/350] private validation Loss: 0.9829, Accuracy: 64.1126\n",
      "Epoch [204/350] public validation Loss: 1.0378, Accuracy: 62.4965\n",
      "learning_rate: 0.0007976644307687256\n",
      "Epoch [205/350] Training Loss: 0.8231, Accuracy: 69.4382\n",
      "Epoch [205/350] private validation Loss: 0.9565, Accuracy: 63.6110\n",
      "Epoch [205/350] public validation Loss: 1.0077, Accuracy: 62.0228\n",
      "learning_rate: 0.000717897987691853\n",
      "Epoch [206/350] Training Loss: 0.8205, Accuracy: 69.6019\n",
      "Epoch [206/350] private validation Loss: 0.9623, Accuracy: 64.9206\n",
      "Epoch [206/350] public validation Loss: 1.0600, Accuracy: 62.6358\n",
      "learning_rate: 0.000717897987691853\n",
      "Epoch [207/350] Training Loss: 0.8202, Accuracy: 69.0515\n",
      "Epoch [207/350] private validation Loss: 0.9940, Accuracy: 63.7782\n",
      "Epoch [207/350] public validation Loss: 1.0075, Accuracy: 61.8278\n",
      "learning_rate: 0.000717897987691853\n",
      "Epoch [208/350] Training Loss: 0.8140, Accuracy: 69.6193\n",
      "Epoch [208/350] private validation Loss: 0.9701, Accuracy: 64.4748\n",
      "Epoch [208/350] public validation Loss: 0.9962, Accuracy: 63.0816\n",
      "learning_rate: 0.000717897987691853\n",
      "Epoch [209/350] Training Loss: 0.8168, Accuracy: 69.5427\n",
      "Epoch [209/350] private validation Loss: 0.9664, Accuracy: 63.9733\n",
      "Epoch [209/350] public validation Loss: 1.0137, Accuracy: 61.5213\n",
      "learning_rate: 0.000717897987691853\n",
      "Epoch [210/350] Training Loss: 0.8191, Accuracy: 69.4974\n",
      "Epoch [210/350] private validation Loss: 0.9628, Accuracy: 64.3633\n",
      "Epoch [210/350] public validation Loss: 1.0112, Accuracy: 62.8030\n",
      "learning_rate: 0.0006461081889226677\n",
      "Epoch [211/350] Training Loss: 0.8166, Accuracy: 69.6611\n",
      "Epoch [211/350] private validation Loss: 0.9790, Accuracy: 64.7534\n",
      "Epoch [211/350] public validation Loss: 0.9866, Accuracy: 62.2179\n",
      "learning_rate: 0.0006461081889226677\n",
      "Epoch [212/350] Training Loss: 0.8153, Accuracy: 69.4556\n",
      "Epoch [212/350] private validation Loss: 0.9718, Accuracy: 64.6420\n",
      "Epoch [212/350] public validation Loss: 1.0124, Accuracy: 62.6916\n",
      "learning_rate: 0.0006461081889226677\n",
      "Epoch [213/350] Training Loss: 0.8168, Accuracy: 69.6367\n",
      "Epoch [213/350] private validation Loss: 0.9543, Accuracy: 63.7225\n",
      "Epoch [213/350] public validation Loss: 0.9993, Accuracy: 61.7721\n",
      "learning_rate: 0.0006461081889226677\n",
      "Epoch [214/350] Training Loss: 0.8137, Accuracy: 69.7830\n",
      "Epoch [214/350] private validation Loss: 0.9705, Accuracy: 64.3076\n",
      "Epoch [214/350] public validation Loss: 1.0138, Accuracy: 62.6637\n",
      "learning_rate: 0.0006461081889226677\n",
      "Epoch [215/350] Training Loss: 0.8124, Accuracy: 69.7377\n",
      "Epoch [215/350] private validation Loss: 0.9646, Accuracy: 64.9206\n",
      "Epoch [215/350] public validation Loss: 0.9930, Accuracy: 61.9671\n",
      "learning_rate: 0.000581497370030401\n",
      "Epoch [216/350] Training Loss: 0.8107, Accuracy: 69.8143\n",
      "saving new model\n",
      "Epoch [216/350] private validation Loss: 0.9275, Accuracy: 65.1992\n",
      "Epoch [216/350] public validation Loss: 0.9856, Accuracy: 62.9702\n",
      "learning_rate: 0.000581497370030401\n",
      "Epoch [217/350] Training Loss: 0.8083, Accuracy: 70.0199\n",
      "Epoch [217/350] private validation Loss: 0.9461, Accuracy: 64.4748\n",
      "Epoch [217/350] public validation Loss: 0.9835, Accuracy: 63.1931\n",
      "learning_rate: 0.000581497370030401\n",
      "Epoch [218/350] Training Loss: 0.8095, Accuracy: 70.0582\n",
      "Epoch [218/350] private validation Loss: 0.9746, Accuracy: 63.4996\n",
      "Epoch [218/350] public validation Loss: 1.0164, Accuracy: 62.0228\n",
      "learning_rate: 0.000581497370030401\n",
      "Epoch [219/350] Training Loss: 0.8091, Accuracy: 69.8143\n",
      "Epoch [219/350] private validation Loss: 0.9992, Accuracy: 64.9763\n",
      "Epoch [219/350] public validation Loss: 1.0304, Accuracy: 62.6080\n",
      "learning_rate: 0.000581497370030401\n",
      "Epoch [220/350] Training Loss: 0.8125, Accuracy: 69.8631\n",
      "Epoch [220/350] private validation Loss: 0.9397, Accuracy: 65.3385\n",
      "Epoch [220/350] public validation Loss: 1.0085, Accuracy: 62.5244\n",
      "learning_rate: 0.000523347633027361\n",
      "Epoch [221/350] Training Loss: 0.8062, Accuracy: 70.0059\n",
      "Epoch [221/350] private validation Loss: 0.9409, Accuracy: 64.2797\n",
      "Epoch [221/350] public validation Loss: 1.0200, Accuracy: 62.3293\n",
      "learning_rate: 0.000523347633027361\n",
      "Epoch [222/350] Training Loss: 0.8050, Accuracy: 69.7900\n",
      "Epoch [222/350] private validation Loss: 0.9749, Accuracy: 65.0599\n",
      "Epoch [222/350] public validation Loss: 0.9920, Accuracy: 62.4408\n",
      "learning_rate: 0.000523347633027361\n",
      "Epoch [223/350] Training Loss: 0.8068, Accuracy: 69.9119\n",
      "Epoch [223/350] private validation Loss: 0.9949, Accuracy: 64.4469\n",
      "Epoch [223/350] public validation Loss: 1.0769, Accuracy: 60.9919\n",
      "learning_rate: 0.000523347633027361\n",
      "Epoch [224/350] Training Loss: 0.8050, Accuracy: 70.0164\n",
      "Epoch [224/350] private validation Loss: 0.9499, Accuracy: 64.7534\n",
      "Epoch [224/350] public validation Loss: 1.0034, Accuracy: 62.0507\n",
      "learning_rate: 0.000523347633027361\n",
      "Epoch [225/350] Training Loss: 0.8070, Accuracy: 70.0199\n",
      "Epoch [225/350] private validation Loss: 0.9515, Accuracy: 64.6977\n",
      "Epoch [225/350] public validation Loss: 1.0306, Accuracy: 62.9145\n",
      "learning_rate: 0.0004710128697246249\n",
      "Epoch [226/350] Training Loss: 0.8033, Accuracy: 70.4901\n",
      "Epoch [226/350] private validation Loss: 0.9442, Accuracy: 65.4221\n",
      "Epoch [226/350] public validation Loss: 1.0008, Accuracy: 62.6916\n",
      "learning_rate: 0.0004710128697246249\n",
      "Epoch [227/350] Training Loss: 0.8020, Accuracy: 70.2846\n",
      "Epoch [227/350] private validation Loss: 0.9448, Accuracy: 64.6420\n",
      "Epoch [227/350] public validation Loss: 1.0226, Accuracy: 62.9145\n",
      "learning_rate: 0.0004710128697246249\n",
      "Epoch [228/350] Training Loss: 0.8058, Accuracy: 70.2950\n",
      "Epoch [228/350] private validation Loss: 0.9486, Accuracy: 65.1156\n",
      "Epoch [228/350] public validation Loss: 0.9955, Accuracy: 63.0259\n",
      "learning_rate: 0.0004710128697246249\n",
      "Epoch [229/350] Training Loss: 0.8005, Accuracy: 70.1453\n",
      "Epoch [229/350] private validation Loss: 0.9554, Accuracy: 65.3385\n",
      "Epoch [229/350] public validation Loss: 1.0333, Accuracy: 62.2179\n",
      "learning_rate: 0.0004710128697246249\n",
      "Epoch [230/350] Training Loss: 0.7991, Accuracy: 70.2219\n",
      "Epoch [230/350] private validation Loss: 0.9644, Accuracy: 63.8618\n",
      "Epoch [230/350] public validation Loss: 0.9927, Accuracy: 63.3881\n",
      "learning_rate: 0.0004239115827521624\n",
      "Epoch [231/350] Training Loss: 0.7986, Accuracy: 70.1557\n",
      "saving new model\n",
      "Epoch [231/350] private validation Loss: 0.9239, Accuracy: 64.5026\n",
      "Epoch [231/350] public validation Loss: 0.9956, Accuracy: 62.7751\n",
      "learning_rate: 0.0004239115827521624\n",
      "Epoch [232/350] Training Loss: 0.7983, Accuracy: 70.3194\n",
      "Epoch [232/350] private validation Loss: 0.9779, Accuracy: 63.7782\n",
      "Epoch [232/350] public validation Loss: 1.0370, Accuracy: 62.8866\n",
      "learning_rate: 0.0004239115827521624\n",
      "Epoch [233/350] Training Loss: 0.7982, Accuracy: 70.3960\n",
      "Epoch [233/350] private validation Loss: 0.9368, Accuracy: 64.9206\n",
      "Epoch [233/350] public validation Loss: 0.9912, Accuracy: 62.6358\n",
      "learning_rate: 0.0004239115827521624\n",
      "Epoch [234/350] Training Loss: 0.7994, Accuracy: 70.2915\n",
      "Epoch [234/350] private validation Loss: 0.9718, Accuracy: 64.6977\n",
      "Epoch [234/350] public validation Loss: 1.0503, Accuracy: 61.5492\n",
      "learning_rate: 0.0004239115827521624\n",
      "Epoch [235/350] Training Loss: 0.7952, Accuracy: 70.3786\n",
      "Epoch [235/350] private validation Loss: 0.9401, Accuracy: 65.1992\n",
      "Epoch [235/350] public validation Loss: 1.0078, Accuracy: 62.4408\n",
      "learning_rate: 0.00038152042447694615\n",
      "Epoch [236/350] Training Loss: 0.7929, Accuracy: 70.4448\n",
      "Epoch [236/350] private validation Loss: 0.9524, Accuracy: 65.4500\n",
      "Epoch [236/350] public validation Loss: 0.9980, Accuracy: 63.1095\n",
      "learning_rate: 0.00038152042447694615\n",
      "Epoch [237/350] Training Loss: 0.7912, Accuracy: 70.6120\n",
      "Epoch [237/350] private validation Loss: 0.9445, Accuracy: 64.8649\n",
      "Epoch [237/350] public validation Loss: 0.9847, Accuracy: 62.7473\n",
      "learning_rate: 0.00038152042447694615\n",
      "Epoch [238/350] Training Loss: 0.7964, Accuracy: 70.4518\n",
      "Epoch [238/350] private validation Loss: 0.9489, Accuracy: 64.8370\n",
      "Epoch [238/350] public validation Loss: 1.0269, Accuracy: 62.6916\n",
      "learning_rate: 0.00038152042447694615\n",
      "Epoch [239/350] Training Loss: 0.7941, Accuracy: 70.3194\n",
      "Epoch [239/350] private validation Loss: 0.9449, Accuracy: 65.0042\n",
      "Epoch [239/350] public validation Loss: 0.9849, Accuracy: 62.6637\n",
      "learning_rate: 0.00038152042447694615\n",
      "Epoch [240/350] Training Loss: 0.7963, Accuracy: 70.5563\n",
      "Epoch [240/350] private validation Loss: 0.9333, Accuracy: 64.9763\n",
      "Epoch [240/350] public validation Loss: 0.9939, Accuracy: 62.3293\n",
      "learning_rate: 0.00034336838202925153\n",
      "Epoch [241/350] Training Loss: 0.7967, Accuracy: 70.5284\n",
      "Epoch [241/350] private validation Loss: 0.9545, Accuracy: 63.8618\n",
      "Epoch [241/350] public validation Loss: 1.0275, Accuracy: 62.6358\n",
      "learning_rate: 0.00034336838202925153\n",
      "Epoch [242/350] Training Loss: 0.7964, Accuracy: 70.5598\n",
      "Epoch [242/350] private validation Loss: 0.9320, Accuracy: 64.5862\n",
      "Epoch [242/350] public validation Loss: 1.0308, Accuracy: 63.1374\n",
      "learning_rate: 0.00034336838202925153\n",
      "Epoch [243/350] Training Loss: 0.7921, Accuracy: 70.5389\n",
      "Epoch [243/350] private validation Loss: 0.9403, Accuracy: 64.6698\n",
      "Epoch [243/350] public validation Loss: 1.0235, Accuracy: 62.4129\n",
      "learning_rate: 0.00034336838202925153\n",
      "Epoch [244/350] Training Loss: 0.7921, Accuracy: 70.5458\n",
      "Epoch [244/350] private validation Loss: 0.9535, Accuracy: 65.4500\n",
      "Epoch [244/350] public validation Loss: 1.0153, Accuracy: 63.4160\n",
      "learning_rate: 0.00034336838202925153\n",
      "Epoch [245/350] Training Loss: 0.7907, Accuracy: 70.7200\n",
      "Epoch [245/350] private validation Loss: 0.9288, Accuracy: 64.8927\n",
      "Epoch [245/350] public validation Loss: 0.9944, Accuracy: 63.3324\n",
      "learning_rate: 0.00030903154382632634\n",
      "Epoch [246/350] Training Loss: 0.7865, Accuracy: 70.8698\n",
      "Epoch [246/350] private validation Loss: 0.9335, Accuracy: 64.4748\n",
      "Epoch [246/350] public validation Loss: 1.0001, Accuracy: 63.1095\n",
      "learning_rate: 0.00030903154382632634\n",
      "Epoch [247/350] Training Loss: 0.7872, Accuracy: 70.7095\n",
      "Epoch [247/350] private validation Loss: 0.9843, Accuracy: 64.7256\n",
      "Epoch [247/350] public validation Loss: 1.0564, Accuracy: 62.5522\n",
      "learning_rate: 0.00030903154382632634\n",
      "Epoch [248/350] Training Loss: 0.7877, Accuracy: 70.8349\n",
      "Epoch [248/350] private validation Loss: 0.9489, Accuracy: 65.5336\n",
      "Epoch [248/350] public validation Loss: 0.9897, Accuracy: 62.6358\n",
      "learning_rate: 0.00030903154382632634\n",
      "Epoch [249/350] Training Loss: 0.7867, Accuracy: 70.7896\n",
      "Epoch [249/350] private validation Loss: 0.9729, Accuracy: 65.7286\n",
      "Epoch [249/350] public validation Loss: 0.9993, Accuracy: 62.4129\n",
      "learning_rate: 0.00030903154382632634\n",
      "Epoch [250/350] Training Loss: 0.7861, Accuracy: 70.6120\n",
      "Epoch [250/350] private validation Loss: 0.9523, Accuracy: 64.9206\n",
      "Epoch [250/350] public validation Loss: 1.0067, Accuracy: 62.6358\n",
      "learning_rate: 0.00027812838944369376\n",
      "Epoch [251/350] Training Loss: 0.7849, Accuracy: 71.0474\n",
      "Epoch [251/350] private validation Loss: 0.9544, Accuracy: 65.6729\n",
      "Epoch [251/350] public validation Loss: 0.9930, Accuracy: 62.4965\n",
      "learning_rate: 0.00027812838944369376\n",
      "Epoch [252/350] Training Loss: 0.7850, Accuracy: 70.8837\n",
      "Epoch [252/350] private validation Loss: 0.9467, Accuracy: 65.5893\n",
      "Epoch [252/350] public validation Loss: 1.0070, Accuracy: 62.7473\n",
      "learning_rate: 0.00027812838944369376\n",
      "Epoch [253/350] Training Loss: 0.7823, Accuracy: 71.1867\n",
      "Epoch [253/350] private validation Loss: 0.9281, Accuracy: 64.8649\n",
      "Epoch [253/350] public validation Loss: 1.0001, Accuracy: 62.3293\n",
      "learning_rate: 0.00027812838944369376\n",
      "Epoch [254/350] Training Loss: 0.7880, Accuracy: 70.5284\n",
      "Epoch [254/350] private validation Loss: 0.9457, Accuracy: 65.1992\n",
      "Epoch [254/350] public validation Loss: 1.0063, Accuracy: 62.4129\n",
      "learning_rate: 0.00027812838944369376\n",
      "Epoch [255/350] Training Loss: 0.7842, Accuracy: 70.7269\n",
      "Epoch [255/350] private validation Loss: 0.9382, Accuracy: 65.1714\n",
      "Epoch [255/350] public validation Loss: 1.0160, Accuracy: 62.8309\n",
      "learning_rate: 0.0002503155504993244\n",
      "Epoch [256/350] Training Loss: 0.7831, Accuracy: 70.9952\n",
      "Epoch [256/350] private validation Loss: 0.9498, Accuracy: 64.3633\n",
      "Epoch [256/350] public validation Loss: 1.0155, Accuracy: 62.7194\n",
      "learning_rate: 0.0002503155504993244\n",
      "Epoch [257/350] Training Loss: 0.7790, Accuracy: 71.1484\n",
      "Epoch [257/350] private validation Loss: 0.9660, Accuracy: 64.5584\n",
      "Epoch [257/350] public validation Loss: 1.0379, Accuracy: 62.7473\n",
      "learning_rate: 0.0002503155504993244\n",
      "Epoch [258/350] Training Loss: 0.7806, Accuracy: 71.2042\n",
      "Epoch [258/350] private validation Loss: 0.9466, Accuracy: 65.7843\n",
      "Epoch [258/350] public validation Loss: 0.9949, Accuracy: 63.1095\n",
      "learning_rate: 0.0002503155504993244\n",
      "Epoch [259/350] Training Loss: 0.7833, Accuracy: 70.9081\n",
      "Epoch [259/350] private validation Loss: 0.9518, Accuracy: 64.4191\n",
      "Epoch [259/350] public validation Loss: 1.0212, Accuracy: 63.1931\n",
      "learning_rate: 0.0002503155504993244\n",
      "Epoch [260/350] Training Loss: 0.7823, Accuracy: 71.1031\n",
      "Epoch [260/350] private validation Loss: 0.9315, Accuracy: 65.4221\n",
      "saving new model\n",
      "Epoch [260/350] public validation Loss: 0.9627, Accuracy: 63.3324\n",
      "learning_rate: 0.00022528399544939195\n",
      "Epoch [261/350] Training Loss: 0.7810, Accuracy: 71.1066\n",
      "Epoch [261/350] private validation Loss: 0.9830, Accuracy: 65.1156\n",
      "Epoch [261/350] public validation Loss: 1.0024, Accuracy: 62.8309\n",
      "learning_rate: 0.00022528399544939195\n",
      "Epoch [262/350] Training Loss: 0.7821, Accuracy: 70.8698\n",
      "Epoch [262/350] private validation Loss: 0.9484, Accuracy: 66.2580\n",
      "Epoch [262/350] public validation Loss: 0.9848, Accuracy: 62.7751\n",
      "learning_rate: 0.00022528399544939195\n",
      "Epoch [263/350] Training Loss: 0.7812, Accuracy: 70.9847\n",
      "Epoch [263/350] private validation Loss: 0.9480, Accuracy: 64.7534\n",
      "Epoch [263/350] public validation Loss: 1.0390, Accuracy: 62.8030\n",
      "learning_rate: 0.00022528399544939195\n",
      "Epoch [264/350] Training Loss: 0.7775, Accuracy: 71.1589\n",
      "Epoch [264/350] private validation Loss: 0.9886, Accuracy: 65.5614\n",
      "Epoch [264/350] public validation Loss: 1.0503, Accuracy: 62.6358\n",
      "learning_rate: 0.00022528399544939195\n",
      "Epoch [265/350] Training Loss: 0.7792, Accuracy: 71.1310\n",
      "Epoch [265/350] private validation Loss: 0.9671, Accuracy: 64.4469\n",
      "Epoch [265/350] public validation Loss: 1.0231, Accuracy: 62.8866\n",
      "learning_rate: 0.00020275559590445276\n",
      "Epoch [266/350] Training Loss: 0.7791, Accuracy: 71.1972\n",
      "Epoch [266/350] private validation Loss: 0.9440, Accuracy: 64.5026\n",
      "Epoch [266/350] public validation Loss: 0.9940, Accuracy: 62.6637\n",
      "learning_rate: 0.00020275559590445276\n",
      "Epoch [267/350] Training Loss: 0.7801, Accuracy: 71.3365\n",
      "Epoch [267/350] private validation Loss: 0.9495, Accuracy: 64.7534\n",
      "Epoch [267/350] public validation Loss: 0.9900, Accuracy: 62.9145\n",
      "learning_rate: 0.00020275559590445276\n",
      "Epoch [268/350] Training Loss: 0.7794, Accuracy: 70.9046\n",
      "Epoch [268/350] private validation Loss: 0.9722, Accuracy: 64.8091\n",
      "Epoch [268/350] public validation Loss: 1.0314, Accuracy: 62.8309\n",
      "learning_rate: 0.00020275559590445276\n",
      "Epoch [269/350] Training Loss: 0.7765, Accuracy: 71.0195\n",
      "Epoch [269/350] private validation Loss: 0.9451, Accuracy: 64.6420\n",
      "Epoch [269/350] public validation Loss: 0.9913, Accuracy: 62.5801\n",
      "learning_rate: 0.00020275559590445276\n",
      "Epoch [270/350] Training Loss: 0.7783, Accuracy: 71.0962\n",
      "Epoch [270/350] private validation Loss: 0.9276, Accuracy: 65.5893\n",
      "Epoch [270/350] public validation Loss: 1.0059, Accuracy: 63.2767\n",
      "learning_rate: 0.0001824800363140075\n",
      "Epoch [271/350] Training Loss: 0.7784, Accuracy: 71.0439\n",
      "Epoch [271/350] private validation Loss: 0.9654, Accuracy: 65.3943\n",
      "Epoch [271/350] public validation Loss: 0.9945, Accuracy: 63.6389\n",
      "learning_rate: 0.0001824800363140075\n",
      "Epoch [272/350] Training Loss: 0.7737, Accuracy: 71.3504\n",
      "Epoch [272/350] private validation Loss: 0.9415, Accuracy: 65.1156\n",
      "Epoch [272/350] public validation Loss: 1.0343, Accuracy: 62.9145\n",
      "learning_rate: 0.0001824800363140075\n",
      "Epoch [273/350] Training Loss: 0.7746, Accuracy: 71.3574\n",
      "Epoch [273/350] private validation Loss: 0.9618, Accuracy: 64.3355\n",
      "Epoch [273/350] public validation Loss: 1.0197, Accuracy: 63.3881\n",
      "learning_rate: 0.0001824800363140075\n",
      "Epoch [274/350] Training Loss: 0.7780, Accuracy: 71.4619\n",
      "Epoch [274/350] private validation Loss: 0.9272, Accuracy: 65.7286\n",
      "Epoch [274/350] public validation Loss: 0.9775, Accuracy: 62.9702\n",
      "learning_rate: 0.0001824800363140075\n",
      "Epoch [275/350] Training Loss: 0.7758, Accuracy: 71.0962\n",
      "Epoch [275/350] private validation Loss: 0.9680, Accuracy: 65.5057\n",
      "Epoch [275/350] public validation Loss: 0.9834, Accuracy: 62.8866\n",
      "learning_rate: 0.00016423203268260675\n",
      "Epoch [276/350] Training Loss: 0.7715, Accuracy: 71.4306\n",
      "Epoch [276/350] private validation Loss: 0.9732, Accuracy: 65.4500\n",
      "Epoch [276/350] public validation Loss: 0.9993, Accuracy: 63.0259\n",
      "learning_rate: 0.00016423203268260675\n",
      "Epoch [277/350] Training Loss: 0.7702, Accuracy: 71.2982\n",
      "Epoch [277/350] private validation Loss: 0.9620, Accuracy: 65.2549\n",
      "Epoch [277/350] public validation Loss: 1.0259, Accuracy: 61.9950\n",
      "learning_rate: 0.00016423203268260675\n",
      "Epoch [278/350] Training Loss: 0.7712, Accuracy: 71.5281\n",
      "Epoch [278/350] private validation Loss: 0.9388, Accuracy: 65.9237\n",
      "Epoch [278/350] public validation Loss: 0.9823, Accuracy: 63.4160\n",
      "learning_rate: 0.00016423203268260675\n",
      "Epoch [279/350] Training Loss: 0.7731, Accuracy: 71.6535\n",
      "Epoch [279/350] private validation Loss: 0.9437, Accuracy: 65.1435\n",
      "Epoch [279/350] public validation Loss: 1.0374, Accuracy: 62.3851\n",
      "learning_rate: 0.00016423203268260675\n",
      "Epoch [280/350] Training Loss: 0.7700, Accuracy: 71.3470\n",
      "Epoch [280/350] private validation Loss: 0.9522, Accuracy: 65.7565\n",
      "Epoch [280/350] public validation Loss: 1.0717, Accuracy: 63.0816\n",
      "learning_rate: 0.00014780882941434607\n",
      "Epoch [281/350] Training Loss: 0.7687, Accuracy: 71.3086\n",
      "Epoch [281/350] private validation Loss: 0.9374, Accuracy: 64.8091\n",
      "Epoch [281/350] public validation Loss: 0.9905, Accuracy: 63.1374\n",
      "learning_rate: 0.00014780882941434607\n",
      "Epoch [282/350] Training Loss: 0.7742, Accuracy: 71.3783\n",
      "saving new model\n",
      "Epoch [282/350] private validation Loss: 0.9222, Accuracy: 65.3943\n",
      "Epoch [282/350] public validation Loss: 0.9932, Accuracy: 62.9145\n",
      "learning_rate: 0.00014780882941434607\n",
      "Epoch [283/350] Training Loss: 0.7724, Accuracy: 71.4027\n",
      "Epoch [283/350] private validation Loss: 0.9380, Accuracy: 65.1992\n",
      "Epoch [283/350] public validation Loss: 0.9836, Accuracy: 63.1931\n",
      "learning_rate: 0.00014780882941434607\n",
      "Epoch [284/350] Training Loss: 0.7758, Accuracy: 71.5176\n",
      "Epoch [284/350] private validation Loss: 0.9584, Accuracy: 65.4778\n",
      "Epoch [284/350] public validation Loss: 1.0478, Accuracy: 62.6637\n",
      "learning_rate: 0.00014780882941434607\n",
      "Epoch [285/350] Training Loss: 0.7693, Accuracy: 71.4758\n",
      "Epoch [285/350] private validation Loss: 0.9500, Accuracy: 65.4221\n",
      "Epoch [285/350] public validation Loss: 1.0154, Accuracy: 63.3045\n",
      "learning_rate: 0.00013302794647291146\n",
      "Epoch [286/350] Training Loss: 0.7705, Accuracy: 71.2982\n",
      "Epoch [286/350] private validation Loss: 0.9343, Accuracy: 64.2240\n",
      "Epoch [286/350] public validation Loss: 0.9854, Accuracy: 62.8309\n",
      "learning_rate: 0.00013302794647291146\n",
      "Epoch [287/350] Training Loss: 0.7686, Accuracy: 71.7301\n",
      "Epoch [287/350] private validation Loss: 0.9528, Accuracy: 65.5057\n",
      "Epoch [287/350] public validation Loss: 0.9704, Accuracy: 62.6637\n",
      "learning_rate: 0.00013302794647291146\n",
      "Epoch [288/350] Training Loss: 0.7664, Accuracy: 71.7057\n",
      "Epoch [288/350] private validation Loss: 0.9228, Accuracy: 65.6450\n",
      "Epoch [288/350] public validation Loss: 1.0070, Accuracy: 63.1652\n",
      "learning_rate: 0.00013302794647291146\n",
      "Epoch [289/350] Training Loss: 0.7677, Accuracy: 71.7406\n",
      "Epoch [289/350] private validation Loss: 0.9695, Accuracy: 64.3633\n",
      "Epoch [289/350] public validation Loss: 1.0171, Accuracy: 63.2210\n",
      "learning_rate: 0.00013302794647291146\n",
      "Epoch [290/350] Training Loss: 0.7720, Accuracy: 71.4131\n",
      "Epoch [290/350] private validation Loss: 0.9464, Accuracy: 64.8649\n",
      "Epoch [290/350] public validation Loss: 1.0460, Accuracy: 63.1652\n",
      "learning_rate: 0.00011972515182562034\n",
      "Epoch [291/350] Training Loss: 0.7720, Accuracy: 71.3261\n",
      "Epoch [291/350] private validation Loss: 0.9651, Accuracy: 64.6698\n",
      "Epoch [291/350] public validation Loss: 1.0306, Accuracy: 62.3851\n",
      "learning_rate: 0.00011972515182562034\n",
      "Epoch [292/350] Training Loss: 0.7675, Accuracy: 71.6187\n",
      "Epoch [292/350] private validation Loss: 0.9423, Accuracy: 64.9763\n",
      "Epoch [292/350] public validation Loss: 1.0128, Accuracy: 63.0538\n",
      "learning_rate: 0.00011972515182562034\n",
      "Epoch [293/350] Training Loss: 0.7648, Accuracy: 71.7266\n",
      "Epoch [293/350] private validation Loss: 0.9671, Accuracy: 65.6172\n",
      "Epoch [293/350] public validation Loss: 1.0177, Accuracy: 63.4996\n",
      "learning_rate: 0.00011972515182562034\n",
      "Epoch [294/350] Training Loss: 0.7705, Accuracy: 71.4097\n",
      "Epoch [294/350] private validation Loss: 0.9714, Accuracy: 64.1404\n",
      "Epoch [294/350] public validation Loss: 1.0388, Accuracy: 63.4717\n",
      "learning_rate: 0.00011972515182562034\n",
      "Epoch [295/350] Training Loss: 0.7680, Accuracy: 71.6570\n",
      "Epoch [295/350] private validation Loss: 0.9416, Accuracy: 65.2549\n",
      "Epoch [295/350] public validation Loss: 1.0223, Accuracy: 63.1095\n",
      "learning_rate: 0.0001077526366430583\n",
      "Epoch [296/350] Training Loss: 0.7662, Accuracy: 71.7266\n",
      "Epoch [296/350] private validation Loss: 0.9661, Accuracy: 64.4469\n",
      "Epoch [296/350] public validation Loss: 1.0021, Accuracy: 63.2210\n",
      "learning_rate: 0.0001077526366430583\n",
      "Epoch [297/350] Training Loss: 0.7675, Accuracy: 71.4271\n",
      "Epoch [297/350] private validation Loss: 0.9746, Accuracy: 63.5832\n",
      "Epoch [297/350] public validation Loss: 1.0277, Accuracy: 63.3881\n",
      "learning_rate: 0.0001077526366430583\n",
      "Epoch [298/350] Training Loss: 0.7656, Accuracy: 71.8346\n",
      "Epoch [298/350] private validation Loss: 0.9860, Accuracy: 64.1126\n",
      "Epoch [298/350] public validation Loss: 1.0181, Accuracy: 63.0259\n",
      "learning_rate: 0.0001077526366430583\n",
      "Epoch [299/350] Training Loss: 0.7714, Accuracy: 71.3539\n",
      "Epoch [299/350] private validation Loss: 0.9407, Accuracy: 64.8091\n",
      "Epoch [299/350] public validation Loss: 1.0138, Accuracy: 63.4160\n",
      "learning_rate: 0.0001077526366430583\n",
      "Epoch [300/350] Training Loss: 0.7674, Accuracy: 71.6117\n",
      "saving new model\n",
      "Epoch [300/350] private validation Loss: 0.9222, Accuracy: 65.8122\n",
      "Epoch [300/350] public validation Loss: 1.0083, Accuracy: 63.2767\n",
      "learning_rate: 9.697737297875247e-05\n",
      "Epoch [301/350] Training Loss: 0.7650, Accuracy: 71.6605\n",
      "Epoch [301/350] private validation Loss: 0.9627, Accuracy: 64.8370\n",
      "Epoch [301/350] public validation Loss: 1.0354, Accuracy: 62.6358\n",
      "learning_rate: 9.697737297875247e-05\n",
      "Epoch [302/350] Training Loss: 0.7665, Accuracy: 71.8172\n",
      "Epoch [302/350] private validation Loss: 0.9661, Accuracy: 64.4748\n",
      "Epoch [302/350] public validation Loss: 1.0280, Accuracy: 62.6916\n",
      "learning_rate: 9.697737297875247e-05\n",
      "Epoch [303/350] Training Loss: 0.7682, Accuracy: 71.3121\n",
      "Epoch [303/350] private validation Loss: 1.0131, Accuracy: 63.0538\n",
      "Epoch [303/350] public validation Loss: 1.0286, Accuracy: 62.8309\n",
      "learning_rate: 9.697737297875247e-05\n",
      "Epoch [304/350] Training Loss: 0.7612, Accuracy: 72.0506\n",
      "Epoch [304/350] private validation Loss: 0.9702, Accuracy: 64.6420\n",
      "Epoch [304/350] public validation Loss: 1.0100, Accuracy: 63.1652\n",
      "learning_rate: 9.697737297875247e-05\n",
      "Epoch [305/350] Training Loss: 0.7680, Accuracy: 71.4166\n",
      "Epoch [305/350] private validation Loss: 0.9532, Accuracy: 65.8122\n",
      "Epoch [305/350] public validation Loss: 1.0302, Accuracy: 63.2210\n",
      "learning_rate: 8.727963568087723e-05\n",
      "Epoch [306/350] Training Loss: 0.7593, Accuracy: 71.8764\n",
      "Epoch [306/350] private validation Loss: 0.9871, Accuracy: 65.2549\n",
      "Epoch [306/350] public validation Loss: 1.0135, Accuracy: 62.1343\n",
      "learning_rate: 8.727963568087723e-05\n",
      "Epoch [307/350] Training Loss: 0.7637, Accuracy: 71.7092\n",
      "Epoch [307/350] private validation Loss: 0.9460, Accuracy: 64.2797\n",
      "Epoch [307/350] public validation Loss: 1.0277, Accuracy: 62.9702\n",
      "learning_rate: 8.727963568087723e-05\n",
      "Epoch [308/350] Training Loss: 0.7641, Accuracy: 71.4584\n",
      "Epoch [308/350] private validation Loss: 0.9445, Accuracy: 66.1187\n",
      "Epoch [308/350] public validation Loss: 1.0134, Accuracy: 62.8587\n",
      "learning_rate: 8.727963568087723e-05\n",
      "Epoch [309/350] Training Loss: 0.7621, Accuracy: 71.8033\n",
      "Epoch [309/350] private validation Loss: 0.9925, Accuracy: 65.4778\n",
      "Epoch [309/350] public validation Loss: 1.0131, Accuracy: 63.1931\n",
      "learning_rate: 8.727963568087723e-05\n",
      "Epoch [310/350] Training Loss: 0.7645, Accuracy: 71.6012\n",
      "Epoch [310/350] private validation Loss: 0.9224, Accuracy: 65.1992\n",
      "Epoch [310/350] public validation Loss: 0.9841, Accuracy: 63.6668\n",
      "learning_rate: 7.855167211278951e-05\n",
      "Epoch [311/350] Training Loss: 0.7621, Accuracy: 71.7963\n",
      "Epoch [311/350] private validation Loss: 0.9733, Accuracy: 65.7286\n",
      "Epoch [311/350] public validation Loss: 1.0069, Accuracy: 63.7503\n",
      "learning_rate: 7.855167211278951e-05\n",
      "Epoch [312/350] Training Loss: 0.7645, Accuracy: 71.5943\n",
      "Epoch [312/350] private validation Loss: 0.9443, Accuracy: 64.2797\n",
      "Epoch [312/350] public validation Loss: 1.0075, Accuracy: 63.0816\n",
      "learning_rate: 7.855167211278951e-05\n",
      "Epoch [313/350] Training Loss: 0.7641, Accuracy: 71.5142\n",
      "Epoch [313/350] private validation Loss: 0.9591, Accuracy: 64.4748\n",
      "Epoch [313/350] public validation Loss: 1.0358, Accuracy: 63.3324\n",
      "learning_rate: 7.855167211278951e-05\n",
      "Epoch [314/350] Training Loss: 0.7633, Accuracy: 71.6047\n",
      "Epoch [314/350] private validation Loss: 0.9888, Accuracy: 64.7256\n",
      "Epoch [314/350] public validation Loss: 1.0119, Accuracy: 62.3851\n",
      "learning_rate: 7.855167211278951e-05\n",
      "Epoch [315/350] Training Loss: 0.7641, Accuracy: 71.8869\n",
      "Epoch [315/350] private validation Loss: 0.9625, Accuracy: 64.4191\n",
      "Epoch [315/350] public validation Loss: 1.0665, Accuracy: 63.1931\n",
      "learning_rate: 7.069650490151055e-05\n",
      "Epoch [316/350] Training Loss: 0.7629, Accuracy: 71.6814\n",
      "Epoch [316/350] private validation Loss: 0.9384, Accuracy: 65.2828\n",
      "Epoch [316/350] public validation Loss: 1.0099, Accuracy: 62.4687\n",
      "learning_rate: 7.069650490151055e-05\n",
      "Epoch [317/350] Training Loss: 0.7566, Accuracy: 72.1969\n",
      "Epoch [317/350] private validation Loss: 0.9633, Accuracy: 65.3943\n",
      "Epoch [317/350] public validation Loss: 0.9851, Accuracy: 62.6637\n",
      "learning_rate: 7.069650490151055e-05\n",
      "Epoch [318/350] Training Loss: 0.7603, Accuracy: 71.9356\n",
      "Epoch [318/350] private validation Loss: 0.9672, Accuracy: 64.8927\n",
      "Epoch [318/350] public validation Loss: 1.0142, Accuracy: 62.8030\n",
      "learning_rate: 7.069650490151055e-05\n",
      "Epoch [319/350] Training Loss: 0.7636, Accuracy: 71.7266\n",
      "Epoch [319/350] private validation Loss: 0.9928, Accuracy: 64.5305\n",
      "Epoch [319/350] public validation Loss: 1.0290, Accuracy: 62.8866\n",
      "learning_rate: 7.069650490151055e-05\n",
      "Epoch [320/350] Training Loss: 0.7610, Accuracy: 71.7371\n",
      "Epoch [320/350] private validation Loss: 0.9650, Accuracy: 65.1435\n",
      "Epoch [320/350] public validation Loss: 1.0045, Accuracy: 63.2210\n",
      "learning_rate: 6.36268544113595e-05\n",
      "Epoch [321/350] Training Loss: 0.7627, Accuracy: 71.6709\n",
      "Epoch [321/350] private validation Loss: 0.9458, Accuracy: 64.8927\n",
      "Epoch [321/350] public validation Loss: 1.0177, Accuracy: 63.6389\n",
      "learning_rate: 6.36268544113595e-05\n",
      "Epoch [322/350] Training Loss: 0.7592, Accuracy: 72.1725\n",
      "Epoch [322/350] private validation Loss: 0.9483, Accuracy: 65.0878\n",
      "Epoch [322/350] public validation Loss: 1.0007, Accuracy: 63.0816\n",
      "learning_rate: 6.36268544113595e-05\n",
      "Epoch [323/350] Training Loss: 0.7638, Accuracy: 71.7510\n",
      "Epoch [323/350] private validation Loss: 0.9536, Accuracy: 65.3107\n",
      "Epoch [323/350] public validation Loss: 0.9996, Accuracy: 63.2767\n",
      "learning_rate: 6.36268544113595e-05\n",
      "Epoch [324/350] Training Loss: 0.7610, Accuracy: 72.0332\n",
      "Epoch [324/350] private validation Loss: 0.9561, Accuracy: 65.1992\n",
      "Epoch [324/350] public validation Loss: 1.0221, Accuracy: 64.0011\n",
      "learning_rate: 6.36268544113595e-05\n",
      "Epoch [325/350] Training Loss: 0.7633, Accuracy: 71.8869\n",
      "Epoch [325/350] private validation Loss: 0.9783, Accuracy: 64.3355\n",
      "Epoch [325/350] public validation Loss: 0.9873, Accuracy: 63.6110\n",
      "learning_rate: 5.726416897022355e-05\n",
      "Epoch [326/350] Training Loss: 0.7601, Accuracy: 71.9983\n",
      "Epoch [326/350] private validation Loss: 0.9520, Accuracy: 65.1435\n",
      "Epoch [326/350] public validation Loss: 1.0277, Accuracy: 63.4996\n",
      "learning_rate: 5.726416897022355e-05\n",
      "Epoch [327/350] Training Loss: 0.7610, Accuracy: 71.9670\n",
      "Epoch [327/350] private validation Loss: 0.9684, Accuracy: 64.3912\n",
      "Epoch [327/350] public validation Loss: 1.0127, Accuracy: 63.2210\n",
      "learning_rate: 5.726416897022355e-05\n",
      "Epoch [328/350] Training Loss: 0.7575, Accuracy: 72.0541\n",
      "Epoch [328/350] private validation Loss: 0.9608, Accuracy: 65.0599\n",
      "Epoch [328/350] public validation Loss: 1.0096, Accuracy: 63.5274\n",
      "learning_rate: 5.726416897022355e-05\n",
      "Epoch [329/350] Training Loss: 0.7588, Accuracy: 71.9391\n",
      "Epoch [329/350] private validation Loss: 0.9431, Accuracy: 64.8649\n",
      "Epoch [329/350] public validation Loss: 1.0109, Accuracy: 63.6110\n",
      "learning_rate: 5.726416897022355e-05\n",
      "Epoch [330/350] Training Loss: 0.7590, Accuracy: 71.9112\n",
      "Epoch [330/350] private validation Loss: 0.9455, Accuracy: 64.7534\n",
      "Epoch [330/350] public validation Loss: 1.0262, Accuracy: 62.4687\n",
      "learning_rate: 5.15377520732012e-05\n",
      "Epoch [331/350] Training Loss: 0.7621, Accuracy: 72.1516\n",
      "Epoch [331/350] private validation Loss: 0.9478, Accuracy: 65.1992\n",
      "Epoch [331/350] public validation Loss: 0.9858, Accuracy: 63.4996\n",
      "learning_rate: 5.15377520732012e-05\n",
      "Epoch [332/350] Training Loss: 0.7602, Accuracy: 71.9496\n",
      "Epoch [332/350] private validation Loss: 0.9857, Accuracy: 65.6172\n",
      "Epoch [332/350] public validation Loss: 1.0166, Accuracy: 62.2179\n",
      "learning_rate: 5.15377520732012e-05\n",
      "Epoch [333/350] Training Loss: 0.7617, Accuracy: 71.9182\n",
      "Epoch [333/350] private validation Loss: 0.9656, Accuracy: 63.9175\n",
      "Epoch [333/350] public validation Loss: 1.0053, Accuracy: 63.4717\n",
      "learning_rate: 5.15377520732012e-05\n",
      "Epoch [334/350] Training Loss: 0.7594, Accuracy: 71.9670\n",
      "Epoch [334/350] private validation Loss: 0.9760, Accuracy: 63.7503\n",
      "Epoch [334/350] public validation Loss: 0.9965, Accuracy: 63.5832\n",
      "learning_rate: 5.15377520732012e-05\n",
      "Epoch [335/350] Training Loss: 0.7583, Accuracy: 71.7719\n",
      "Epoch [335/350] private validation Loss: 0.9938, Accuracy: 65.7008\n",
      "Epoch [335/350] public validation Loss: 0.9862, Accuracy: 62.8866\n",
      "learning_rate: 4.6383976865881087e-05\n",
      "Epoch [336/350] Training Loss: 0.7584, Accuracy: 72.1481\n",
      "Epoch [336/350] private validation Loss: 0.9701, Accuracy: 65.5893\n",
      "Epoch [336/350] public validation Loss: 1.0214, Accuracy: 62.9423\n",
      "learning_rate: 4.6383976865881087e-05\n",
      "Epoch [337/350] Training Loss: 0.7586, Accuracy: 72.0297\n",
      "Epoch [337/350] private validation Loss: 1.0580, Accuracy: 62.8587\n",
      "Epoch [337/350] public validation Loss: 1.0970, Accuracy: 61.6328\n",
      "learning_rate: 4.6383976865881087e-05\n",
      "Epoch [338/350] Training Loss: 0.7618, Accuracy: 71.6361\n",
      "Epoch [338/350] private validation Loss: 0.9568, Accuracy: 65.1714\n",
      "Epoch [338/350] public validation Loss: 1.0199, Accuracy: 63.0259\n",
      "learning_rate: 4.6383976865881087e-05\n",
      "Epoch [339/350] Training Loss: 0.7595, Accuracy: 71.9635\n",
      "Epoch [339/350] private validation Loss: 0.9636, Accuracy: 64.1404\n",
      "Epoch [339/350] public validation Loss: 1.0563, Accuracy: 62.7751\n",
      "learning_rate: 4.6383976865881087e-05\n",
      "Epoch [340/350] Training Loss: 0.7558, Accuracy: 72.1446\n",
      "Epoch [340/350] private validation Loss: 0.9635, Accuracy: 64.1126\n",
      "Epoch [340/350] public validation Loss: 1.0082, Accuracy: 62.6637\n",
      "learning_rate: 4.174557917929297e-05\n",
      "Epoch [341/350] Training Loss: 0.7595, Accuracy: 71.8416\n",
      "Epoch [341/350] private validation Loss: 0.9519, Accuracy: 65.0878\n",
      "Epoch [341/350] public validation Loss: 0.9832, Accuracy: 63.2767\n",
      "learning_rate: 4.174557917929297e-05\n",
      "Epoch [342/350] Training Loss: 0.7579, Accuracy: 71.8555\n",
      "Epoch [342/350] private validation Loss: 0.9921, Accuracy: 65.3664\n",
      "Epoch [342/350] public validation Loss: 1.0236, Accuracy: 63.4439\n",
      "learning_rate: 4.174557917929297e-05\n",
      "Epoch [343/350] Training Loss: 0.7621, Accuracy: 71.8068\n",
      "Epoch [343/350] private validation Loss: 0.9541, Accuracy: 65.4500\n",
      "Epoch [343/350] public validation Loss: 1.0475, Accuracy: 63.1374\n",
      "learning_rate: 4.174557917929297e-05\n",
      "Epoch [344/350] Training Loss: 0.7577, Accuracy: 72.1620\n",
      "Epoch [344/350] private validation Loss: 0.9451, Accuracy: 64.7534\n",
      "Epoch [344/350] public validation Loss: 0.9845, Accuracy: 63.4717\n",
      "learning_rate: 4.174557917929297e-05\n",
      "Epoch [345/350] Training Loss: 0.7598, Accuracy: 71.6848\n",
      "Epoch [345/350] private validation Loss: 0.9238, Accuracy: 65.8401\n",
      "Epoch [345/350] public validation Loss: 1.0280, Accuracy: 63.0259\n",
      "learning_rate: 3.757102126136367e-05\n",
      "Epoch [346/350] Training Loss: 0.7577, Accuracy: 72.0053\n",
      "Epoch [346/350] private validation Loss: 0.9477, Accuracy: 64.7813\n",
      "Epoch [346/350] public validation Loss: 0.9986, Accuracy: 63.5832\n",
      "learning_rate: 3.757102126136367e-05\n",
      "Epoch [347/350] Training Loss: 0.7552, Accuracy: 72.0924\n",
      "Epoch [347/350] private validation Loss: 0.9432, Accuracy: 65.7843\n",
      "Epoch [347/350] public validation Loss: 0.9824, Accuracy: 63.2488\n",
      "learning_rate: 3.757102126136367e-05\n",
      "Epoch [348/350] Training Loss: 0.7600, Accuracy: 71.9635\n",
      "Epoch [348/350] private validation Loss: 0.9616, Accuracy: 64.3633\n",
      "Epoch [348/350] public validation Loss: 0.9979, Accuracy: 63.0816\n",
      "learning_rate: 3.757102126136367e-05\n",
      "Epoch [349/350] Training Loss: 0.7517, Accuracy: 72.1551\n",
      "Epoch [349/350] private validation Loss: 0.9623, Accuracy: 65.5893\n",
      "Epoch [349/350] public validation Loss: 1.0359, Accuracy: 62.8587\n",
      "learning_rate: 3.757102126136367e-05\n",
      "Epoch [350/350] Training Loss: 0.7541, Accuracy: 72.2561\n",
      "Epoch [350/350] private validation Loss: 0.9581, Accuracy: 65.0599\n",
      "Epoch [350/350] public validation Loss: 1.0409, Accuracy: 63.6668\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import csv\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "shape = (44, 44)\n",
    "\n",
    "\n",
    "class DataSetFactory:\n",
    "\n",
    "    def __init__(self):\n",
    "        images = []\n",
    "        emotions = []\n",
    "        private_images = []\n",
    "        private_emotions = []\n",
    "        public_images = []\n",
    "        public_emotions = []\n",
    "\n",
    "        with open('fer2013.csv', 'r') as csvin:\n",
    "            data = csv.reader(csvin)\n",
    "            next(data)\n",
    "            for row in data:\n",
    "                face = [int(pixel) for pixel in row[1].split()]\n",
    "                face = np.asarray(face).reshape(48, 48)\n",
    "                face = face.astype('uint8')\n",
    "\n",
    "                if row[-1] == 'Training':\n",
    "                    emotions.append(int(row[0]))\n",
    "                    images.append(Image.fromarray(face))\n",
    "                elif row[-1] == \"PrivateTest\":\n",
    "                    private_emotions.append(int(row[0]))\n",
    "                    private_images.append(Image.fromarray(face))\n",
    "                elif row[-1] == \"PublicTest\":\n",
    "                    public_emotions.append(int(row[0]))\n",
    "                    public_images.append(Image.fromarray(face))\n",
    "\n",
    "        print('training size %d : private val size %d : public val size %d' % (\n",
    "            len(images), len(private_images), len(public_images)))\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(shape[0]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.CenterCrop(shape[0]),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.training = DataSet(transform=train_transform, images=images, emotions=emotions)\n",
    "        self.private = DataSet(transform=val_transform, images=private_images, emotions=private_emotions)\n",
    "        self.public = DataSet(transform=val_transform, images=public_images, emotions=public_emotions)\n",
    "\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, transform=None, images=None, emotions=None):\n",
    "        self.transform = transform\n",
    "        self.images = images\n",
    "        self.emotions = emotions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        emotion = self.emotions[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, emotion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # variables  -------------\n",
    "    batch_size = 128\n",
    "    lr = 0.01\n",
    "    epochs = 350\n",
    "    learning_rate_decay_start = 80\n",
    "    learning_rate_decay_every = 5\n",
    "    learning_rate_decay_rate = 0.9\n",
    "    # ------------------------\n",
    "\n",
    "    classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    network = Model(num_classes=len(classes)).to(device)\n",
    "    if not torch.cuda.is_available():\n",
    "        summary(network, (1, shape[0], shape[1]))\n",
    "\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr, momentum=0.9, weight_decay=5e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    factory = DataSetFactory()\n",
    "\n",
    "    training_loader = DataLoader(factory.training, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    validation_loader = {\n",
    "        'private': DataLoader(factory.private, batch_size=batch_size, shuffle=True, num_workers=1),\n",
    "        'public': DataLoader(factory.public, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    }\n",
    "\n",
    "    min_validation_loss = {\n",
    "        'private': 10000,\n",
    "        'public': 10000,\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        network.train()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_train_loss = 0\n",
    "        if epoch > learning_rate_decay_start and learning_rate_decay_start >= 0:\n",
    "\n",
    "            #\n",
    "            frac = (epoch - learning_rate_decay_start) // learning_rate_decay_every\n",
    "            decay_factor = learning_rate_decay_rate ** frac\n",
    "            current_lr = lr * decay_factor\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = current_lr\n",
    "        else:\n",
    "            current_lr = lr\n",
    "\n",
    "        print('learning_rate: %s' % str(current_lr))\n",
    "        for i, (x_train, y_train) in enumerate(training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            y_predicted = network(x_train)\n",
    "            loss = criterion(y_predicted, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(y_predicted.data, 1)\n",
    "            total_train_loss += loss.data\n",
    "            total += y_train.size(0)\n",
    "            correct += predicted.eq(y_train.data).sum()\n",
    "        accuracy = 100. * float(correct) / total\n",
    "        print('Epoch [%d/%d] Training Loss: %.4f, Accuracy: %.4f' % (\n",
    "            epoch + 1, epochs, total_train_loss / (i + 1), accuracy))\n",
    "\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            for name in ['private', 'public']:\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                total_validation_loss = 0\n",
    "                for j, (x_val, y_val) in enumerate(validation_loader[name]):\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    y_val_predicted = network(x_val)\n",
    "                    val_loss = criterion(y_val_predicted, y_val)\n",
    "                    _, predicted = torch.max(y_val_predicted.data, 1)\n",
    "                    total_validation_loss += val_loss.data\n",
    "                    total += y_val.size(0)\n",
    "                    correct += predicted.eq(y_val.data).sum()\n",
    "\n",
    "                accuracy = 100. * float(correct) / total\n",
    "                if total_validation_loss <= min_validation_loss[name]:\n",
    "                    if epoch >= 10:\n",
    "                        print('saving new model')\n",
    "                        state = {'net': network.state_dict()}\n",
    "                        torch.save(state, '%s_model_%d_%d.t7' % (name, epoch + 1, accuracy))\n",
    "                    min_validation_loss[name] = total_validation_loss\n",
    "\n",
    "                print('Epoch [%d/%d] %s validation Loss: %.4f, Accuracy: %.4f' % (\n",
    "                    epoch + 1, epochs, name, total_validation_loss / (j + 1), accuracy))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes the Model and the output, this was trained on corei5 2.3ghz intel processor.\n",
    "<b>About the model</b><br>\n",
    "\n",
    "\n",
    "\n",
    "This model will process in batches of 128 items per epoch <a href=\"https://radiopaedia.org/articles/epoch-machine-learning#:~:text=An%20epoch%20is%20a%20term,of%20data%20is%20very%20large\">(you can find definition of these terms here)</a>\n",
    "\n",
    "The learning rate will start at 0.1 but it will be optimized with SGD optimizer to make the model optimize better and also Crossenthropyloss function is also chosen for this particular task.\n",
    "The images are also fliped horizontally and cropped periodically and randomly and also converted to tensor.\n",
    "\n",
    "The output of the model will give out the training loss, accuracy based on the overall performance and also based on the fer2013.csv data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hE1MoSM0urEn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class _BaseWrapper(object):\n",
    "  \n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(_BaseWrapper, self).__init__()\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model = model\n",
    "        self.handlers = []  # a set of hook function handlers\n",
    "\n",
    "    def _encode_one_hot(self, ids):\n",
    "        one_hot = torch.zeros_like(self.logits).to(self.device)\n",
    "        one_hot.scatter_(1, ids, 1.0)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, image):\n",
    "       \n",
    "        self.model.zero_grad()\n",
    "        self.logits = self.model(image)\n",
    "        self.probs = F.softmax(self.logits, dim=1)\n",
    "        return self.probs.sort(dim=1, descending=True)\n",
    "\n",
    "    def backward(self, ids):\n",
    "      \n",
    "\n",
    "        one_hot = self._encode_one_hot(ids)\n",
    "        self.logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "    def generate(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def remove_hook(self):\n",
    "       \n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "class BackPropagation(_BaseWrapper):\n",
    "    def forward(self, image):\n",
    "        self.image = image.requires_grad_()\n",
    "        return super(BackPropagation, self).forward(self.image)\n",
    "\n",
    "    def generate(self):\n",
    "        gradient = self.image.grad.clone()\n",
    "        self.image.grad.zero_()\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class GuidedBackPropagation(BackPropagation):\n",
    "  \n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackPropagation, self).__init__(model)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "           \n",
    "            if isinstance(module, nn.ReLU):\n",
    "                return (torch.clamp(grad_in[0], min=0.0),)\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            self.handlers.append(module[1].register_backward_hook(backward_hook))\n",
    "\n",
    "\n",
    "class GradCAM(_BaseWrapper):\n",
    "   \n",
    "\n",
    "    def __init__(self, model, candidate_layers=None):\n",
    "        super(GradCAM, self).__init__(model)\n",
    "        self.fmap_pool = OrderedDict()\n",
    "        self.grad_pool = OrderedDict()\n",
    "        self.candidate_layers = candidate_layers  # list\n",
    "\n",
    "        def forward_hook(key):\n",
    "            def forward_hook_(module, input, output):\n",
    "                # Save feature maps\n",
    "                self.fmap_pool[key] = output.detach()\n",
    "\n",
    "            return forward_hook_\n",
    "\n",
    "        def backward_hook(key):\n",
    "            def backward_hook_(module, grad_in, grad_out):\n",
    "                # Save the gradients correspond to the featuremaps\n",
    "                self.grad_pool[key] = grad_out[0].detach()\n",
    "\n",
    "            return backward_hook_\n",
    "\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if self.candidate_layers is None or name in self.candidate_layers:\n",
    "                self.handlers.append(module.register_forward_hook(forward_hook(name)))\n",
    "                self.handlers.append(module.register_backward_hook(backward_hook(name)))\n",
    "\n",
    "    def _find(self, pool, target_layer):\n",
    "        if target_layer in pool.keys():\n",
    "            return pool[target_layer]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer name: {}\".format(target_layer))\n",
    "\n",
    "    def _compute_grad_weights(self, grads):\n",
    "        return F.adaptive_avg_pool2d(grads, 1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.image_shape = image.shape[2:]\n",
    "        return super(GradCAM, self).forward(image)\n",
    "\n",
    "    def generate(self, target_layer):\n",
    "        fmaps = self._find(self.fmap_pool, target_layer)\n",
    "        grads = self._find(self.grad_pool, target_layer)\n",
    "        weights = self._compute_grad_weights(grads)\n",
    "\n",
    "        gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n",
    "        gcam = F.relu(gcam)\n",
    "\n",
    "        gcam = F.interpolate(\n",
    "            gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        B, C, H, W = gcam.shape\n",
    "        gcam = gcam.view(B, -1)\n",
    "        gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
    "        gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
    "        gcam = gcam.view(B, C, H, W)\n",
    "\n",
    "        return gcam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import torch.hub\n",
    "import os\n",
    "import model\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from visualize.grad_cam import BackPropagation, GradCAM,GuidedBackPropagation\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('./visualize/haarcascade_frontalface_default.xml')\n",
    "shape = (48,48)\n",
    "classes = [\n",
    "    'Angry',\n",
    "    'Disgust',\n",
    "    'Fear',\n",
    "    'Happy',\n",
    "    'Sad',\n",
    "    'Surprised',\n",
    "    'Neutral'\n",
    "]\n",
    "\n",
    "def preprocess(image_path):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = cv2.imread(image_path)\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        image,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(1, 1),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        print('no face found')\n",
    "        face = cv2.resize(image, shape)\n",
    "    else:\n",
    "        (x, y, w, h) = faces[0]\n",
    "        face = image[y:y + h, x:x + w]\n",
    "        face = cv2.resize(face, shape)\n",
    "\n",
    "    img = Image.fromarray(face).convert('L')\n",
    "    inputs = transform_test(img)\n",
    "    return inputs, face\n",
    "\n",
    "\n",
    "def get_gradient_image(gradient):\n",
    "    gradient = gradient.cpu().numpy().transpose(1, 2, 0)\n",
    "    gradient -= gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    gradient *= 255.0\n",
    "    return np.uint8(gradient)\n",
    "\n",
    "\n",
    "def get_gradcam_image(gcam, raw_image, paper_cmap=False):\n",
    "    gcam = gcam.cpu().numpy()\n",
    "    cmap = cm.jet_r(gcam)[..., :3] * 255.0\n",
    "    if paper_cmap:\n",
    "        alpha = gcam[..., None]\n",
    "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "    else:\n",
    "        gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n",
    "    return np.uint8(gcam)\n",
    "\n",
    "\n",
    "def guided_backprop(images, model_name):\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        target, raw_image = preprocess(image['path'])\n",
    "        image['image'] = target\n",
    "        image['raw_image'] = raw_image\n",
    "\n",
    "    net = model.Model(num_classes=len(classes))\n",
    "    checkpoint = torch.load(os.path.join('../trained', model_name), map_location=torch.device('cpu'))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    net.eval()\n",
    "    #summary(net, (1, shape[0], shape[1]))\n",
    "\n",
    "    result_images = []\n",
    "    for index, image in enumerate(images):\n",
    "        img = torch.stack([image['image']])\n",
    "        bp = BackPropagation(model=net)\n",
    "        probs, ids = bp.forward(img)\n",
    "        gcam = GradCAM(model=net)\n",
    "        _ = gcam.forward(img)\n",
    "\n",
    "        gbp = GuidedBackPropagation(model=net)\n",
    "        _ = gbp.forward(img)\n",
    "\n",
    "        # Guided Backpropagation\n",
    "        actual_emotion = ids[:,0]\n",
    "        gbp.backward(ids=actual_emotion.reshape(1,1))\n",
    "        gradients = gbp.generate()\n",
    "\n",
    "        # Grad-CAM\n",
    "        gcam.backward(ids=actual_emotion.reshape(1,1))\n",
    "        regions = gcam.generate(target_layer='last_conv')\n",
    "\n",
    "        # Get Images\n",
    "        label_image = np.zeros((shape[0],65, 3), np.uint8)\n",
    "        cv2.putText(label_image, classes[actual_emotion.data], (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        prob_image = np.zeros((shape[0],60,3), np.uint8)\n",
    "        cv2.putText(prob_image, '%.1f%%' % (probs.data[:,0] * 100), (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        guided_bpg_image = get_gradient_image(gradients[0])\n",
    "        guided_bpg_image = cv2.merge((guided_bpg_image, guided_bpg_image, guided_bpg_image))\n",
    "\n",
    "        grad_cam_image = get_gradcam_image(gcam=regions[0, 0],raw_image=image['raw_image'])\n",
    "\n",
    "        guided_gradcam_image = get_gradient_image(torch.mul(regions, gradients)[0])\n",
    "        guided_gradcam_image = cv2.merge((guided_gradcam_image, guided_gradcam_image, guided_gradcam_image))\n",
    "\n",
    "        img = cv2.hconcat([image['raw_image'],label_image,prob_image,guided_bpg_image,grad_cam_image,guided_gradcam_image])\n",
    "        result_images.append(img)\n",
    "        #print(image['path'],classes[actual_emotion.data], probs.data[:,0] * 100)\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.imshow(prob_image, cmap = 'gray', interpolation = 'bicubic')\n",
    "        plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        plt.show()\n",
    "    #cv2.imwrite('../test/guided_gradcam.jpg',cv2.resize(cv2.vconcat(result_images), None, fx=2,fy=2))\n",
    "\n",
    "\n",
    "def main():\n",
    "    guided_backprop(\n",
    "        images=[\n",
    "            #{'path': '../test/angry.jpg'},\n",
    "            #{'path': '../test/happy.jpg'},\n",
    "            #{'path': '../test/sad.jpg'},\n",
    "            {'path': '../test/surprised.jpg'},\n",
    "        ],\n",
    "        model_name='private_model_233_66.t7'\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Code cell is responsible for feature extraction from an input image, and its provides the bright parts of the image.\n",
    "\n",
    "In essence,  GuidedBackPropagation, neurons that have no effects or negative effects on the prediction value of a target class are masked out and ignored. By doing so, we can prevent the flow of gradients through such neurons, resulting in less noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test</b><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This code cell is the test case based on the images provided in the test folder, the four images are fed into the model,and the result saved as generated.jpg into the test folder.\n",
    "\n",
    "We load our model here,  model_name='private_model_233_66.t7' this is the output of the trained model based on the best accuracy and performance. When we train the model its always saving in this file format, there are several files in the the trained folder, this was chosen because at this instance, the model was at its best accuracy based on the model output.\n",
    "\n",
    "Its then loaded and  the four images are passed through the extracted features of the model, and the result is stored into the /test folder as generated.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzwrOa7seYFF"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('../test/guided_gradcam.jpg',0)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated image is now ploted to be easily accessible through cv2 and matplotlib libries\n",
    "\n",
    "<b>Performance</b>\n",
    "\n",
    "As from the plotted image above, the model had 99.7% accuracy on the angry inputs,93.8% accuracy on the happy inputs,91.3% accuracy on the sad inputs,98.0% accuracy on the surprised inputs.\n",
    "\n",
    "From the given aims and objectives, the model performed as expected.\n",
    "\n",
    "\n",
    "The code can be used for different functionalitie which need more user information to be detected automatically just from the facial picture or expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-57ebddede66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# captures frame and returns boolean value and captured image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import cv2  \n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "#load model  \n",
    "model='private_model_233_66.t7' \n",
    "#load weights  \n",
    "\n",
    "  \n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  \n",
    "  \n",
    "cap=cv2.VideoCapture(0)  \n",
    "  \n",
    "while True:  \n",
    "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
    "    if not ret:  \n",
    "        continue  \n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
    "  \n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)  \n",
    "  \n",
    "  \n",
    "    for (x,y,w,h) in faces_detected:  \n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
    "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
    "        img_pixels = image.img_to_array(roi_gray)  \n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
    "        img_pixels /= 255  \n",
    "  \n",
    "        import os.path as osp\n",
    "\n",
    "        import cv2\n",
    "        import matplotlib.cm as cm\n",
    "        import numpy as np\n",
    "        import torch.hub\n",
    "        import os\n",
    "        import model\n",
    "        from PIL import Image\n",
    "        from torchvision import transforms\n",
    "        from torchsummary import summary\n",
    "        from visualize.grad_cam import BackPropagation, GradCAM,GuidedBackPropagation\n",
    "\n",
    "        faceCascade = cv2.CascadeClassifier('./visualize/haarcascade_frontalface_default.xml')\n",
    "        shape = (48,48)\n",
    "        classes = [\n",
    "       'Angry',\n",
    "       'Disgust',\n",
    "       'Fear',\n",
    "       'Happy',\n",
    "       'Sad',\n",
    "       'Surprised',\n",
    "       'Neutral'\n",
    "        ]\n",
    "\n",
    "        def preprocess(image_path):\n",
    "            transform_test = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image = cv2.imread(image_path)\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            image,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(1, 1),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            print('no face found')\n",
    "            face = cv2.resize(image, shape)\n",
    "        else:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            face = image[y:y + h, x:x + w]\n",
    "            face = cv2.resize(face, shape)\n",
    "\n",
    "        img = Image.fromarray(face).convert('L')\n",
    "        inputs = transform_test(img)\n",
    "    #return inputs, face\n",
    "\n",
    "\n",
    "    def get_gradient_image(gradient):\n",
    "        gradient = gradient.cpu().numpy().transpose(1, 2, 0)\n",
    "        gradient -= gradient.min()\n",
    "        gradient /= gradient.max()\n",
    "        gradient *= 255.0\n",
    "        return np.uint8(gradient)\n",
    "\n",
    "\n",
    "    def get_gradcam_image(gcam, raw_image, paper_cmap=False):\n",
    "        gcam = gcam.cpu().numpy()\n",
    "        cmap = cm.jet_r(gcam)[..., :3] * 255.0\n",
    "        if paper_cmap:\n",
    "            alpha = gcam[..., None]\n",
    "            gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "        else:\n",
    "            gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n",
    "            return np.uint8(gcam)\n",
    "\n",
    "\n",
    "    def guided_backprop(cap, model_name):\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            target, raw_image = preprocess(image['path'])\n",
    "            image['image'] = target\n",
    "            image['raw_image'] = raw_image\n",
    "\n",
    "        net = model.Model(num_classes=len(classes))\n",
    "        checkpoint = torch.load(os.path.join('../trained', model_name), map_location=torch.device('cpu'))\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        net.eval()\n",
    "    #summary(net, (1, shape[0], shape[1]))\n",
    "\n",
    "        result_images = []\n",
    "        for index, image in enumerate(images):\n",
    "            img = torch.stack([image['image']])\n",
    "            bp = BackPropagation(model=net)\n",
    "            probs, ids = bp.forward(img)\n",
    "            gcam = GradCAM(model=net)\n",
    "            _ = gcam.forward(img)\n",
    "\n",
    "            gbp = GuidedBackPropagation(model=net)\n",
    "            _ = gbp.forward(img)\n",
    "\n",
    "        # Guided Backpropagation\n",
    "            actual_emotion = ids[:,0]\n",
    "            gbp.backward(ids=actual_emotion.reshape(1,1))\n",
    "            gradients = gbp.generate()\n",
    "\n",
    "        # Grad-CAM\n",
    "            gcam.backward(ids=actual_emotion.reshape(1,1))\n",
    "            regions = gcam.generate(target_layer='last_conv')\n",
    "\n",
    "        # Get Images\n",
    "            label_image = np.zeros((shape[0],65, 3), np.uint8)\n",
    "            cv2.putText(label_image, classes[actual_emotion.data], (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            prob_image = np.zeros((shape[0],60,3), np.uint8)\n",
    "            cv2.putText(prob_image, '%.1f%%' % (probs.data[:,0] * 100), (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            guided_bpg_image = get_gradient_image(gradients[0])\n",
    "            guided_bpg_image = cv2.merge((guided_bpg_image, guided_bpg_image, guided_bpg_image))\n",
    "\n",
    "            grad_cam_image = get_gradcam_image(gcam=regions[0, 0],raw_image=image['raw_image'])\n",
    "\n",
    "            guided_gradcam_image = get_gradient_image(torch.mul(regions, gradients)[0])\n",
    "            guided_gradcam_image = cv2.merge((guided_gradcam_image, guided_gradcam_image, guided_gradcam_image))\n",
    "\n",
    "            img = cv2.hconcat([image['raw_image'],label_image,prob_image,guided_bpg_image,grad_cam_image,guided_gradcam_image])\n",
    "            result_images.append(img)\n",
    "         #print(image['path'],classes[actual_emotion.data], probs.data[:,0] * 100)\n",
    "            from matplotlib import pyplot as plt\n",
    "            plt.imshow(prob_image, cmap = 'gray', interpolation = 'bicubic')\n",
    "            plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "            plt.show()\n",
    "    #cv2.imwrite('../test/guided_gradcam.jpg',cv2.resize(cv2.vconcat(result_images), None, fx=2,fy=2))\n",
    "\n",
    "\n",
    "    def main():\n",
    "        guided_backprop(\n",
    "            images=[\n",
    "                #{'path': '../test/angry.jpg'},\n",
    "                #{'path': '../test/happy.jpg'},\n",
    "                #{'path': '../test/sad.jpg'},\n",
    "                {'path': '../test/surprised.jpg'},\n",
    "            ],\n",
    "            model_name='private_model_233_66.t7'\n",
    "       )\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "        #find max indexed array  \n",
    "        #max_index = np.argmax(predictions[0])  \n",
    "  \n",
    "        #emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
    "        #predicted_emotion = emotions[max_index]  \n",
    "  \n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
    "  \n",
    "    resized_img = cv2.resize(test_img, (1000, 700))  \n",
    "    cv2.imshow('Facial emotion analysis ',resized_img)  \n",
    "  \n",
    "  \n",
    "  \n",
    "    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed  \n",
    "        break  \n",
    "cap.release()  \n",
    "cv2.destroyAllWindows  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import cv2  \n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "#load model  \n",
    "model='private_model_233_66.t7' \n",
    "#load weights  \n",
    "\n",
    "  \n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  \n",
    "  \n",
    "cap=cv2.VideoCapture(0)  \n",
    "  \n",
    "while True:  \n",
    "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
    "    if not ret:  \n",
    "        continue  \n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
    "  \n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)  \n",
    "  \n",
    "  \n",
    "    for (x,y,w,h) in faces_detected:  \n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
    "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
    "        img_pixels = image.img_to_array(roi_gray)  \n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
    "        img_pixels /= 255  \n",
    "  \n",
    "        predictions = model(img_pixels)  \n",
    "  \n",
    "        #find max indexed array  \n",
    "        #max_index = np.argmax(predictions[0])  \n",
    "  \n",
    "        #emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
    "        #predicted_emotion = emotions[max_index]  \n",
    "  \n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
    "  \n",
    "    resized_img = cv2.resize(test_img, (1000, 700))  \n",
    "    cv2.imshow('Facial emotion analysis ',resized_img)  \n",
    "  \n",
    "  \n",
    "  \n",
    "    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed  \n",
    "        break  \n",
    "cap.release()  \n",
    "cv2.destroyAllWindows  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
